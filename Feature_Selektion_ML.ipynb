{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5529d6eb-4142-4872-8491-fd9cca9c8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "def repeated_k_fold(model, X, y, n_splits=5, n_repeats=10):\n",
    "    \"\"\"Führt repeated k-fold cross-validation durch und berechnet die Metriken.\"\"\"\n",
    "    rkf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
    "\n",
    "    accuracy_train, accuracy_test = [], []\n",
    "    f1, recall, roc_auc = [], [], []\n",
    "\n",
    "    for train_index, test_index in rkf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        accuracy_train.append(accuracy_score(y_train, y_train_pred))\n",
    "        accuracy_test.append(accuracy_score(y_test, y_test_pred))\n",
    "        f1.append(f1_score(y_test, y_test_pred))\n",
    "        recall.append(recall_score(y_test, y_test_pred))\n",
    "        roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "    return {\n",
    "        \"Train Accuracy\": (np.mean(accuracy_train), np.std(accuracy_train)),\n",
    "        \"Test Accuracy\": (np.mean(accuracy_test), np.std(accuracy_test)),\n",
    "        \"F1-Score_1\": (np.mean(f1), np.std(f1)),\n",
    "        \"Recall_1\": (np.mean(recall), np.std(recall)),\n",
    "        \"ROC-AUC_1\": (np.mean(roc_auc), np.std(roc_auc)),\n",
    "    }\n",
    "\n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\Basistabelle\\Basistabelle_ML.xlsx\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Zielvariable (y) und Features (X) extrahieren\n",
    "    y = df['Verletzungsstatus']\n",
    "    \n",
    "    # Dummy-Variable \"Geschlecht_weiblich\" separieren\n",
    "    if 'Geschlecht_weiblich' in df.columns:\n",
    "        geschlecht_weiblich = df[['Geschlecht_weiblich']]\n",
    "        X = df.drop(columns=['Verletzungsstatus', 'Geschlecht_weiblich'])\n",
    "    else:\n",
    "        X = df.drop(columns=['Verletzungsstatus'])\n",
    "        geschlecht_weiblich = None  \n",
    "\n",
    "    # Skalierung der Features (ohne \"Geschlecht_weiblich\")\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Falls vorhanden, die Dummy-Variable wieder anhängen\n",
    "    if geschlecht_weiblich is not None:\n",
    "        X_scaled = np.hstack((X_scaled, geschlecht_weiblich.values))\n",
    "\n",
    "    # Modelle definieren\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(eval_metric=\"logloss\", random_state=42),\n",
    "        \"LightGBM\": LGBMClassifier(verbose=-1, random_state=42),\n",
    "        \"SVC\": SVC(probability=True, random_state=42),\n",
    "        \"k-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "        \"MLP Classifier\": MLPClassifier(max_iter=1000, random_state=42),\n",
    "        \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "        \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(),\n",
    "        \"Quadratic Discriminant Analysis\": QuadraticDiscriminantAnalysis(),\n",
    "        \"Bagging Classifier\": BaggingClassifier(random_state=42),\n",
    "        \"Extra Trees\": ExtraTreesClassifier(random_state=42),\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Modell wird validiert: {model_name}\")\n",
    "        metrics = repeated_k_fold(model, X_scaled, y)  \n",
    "        \n",
    "        # Formatierung der Ergebnisse mit ±\n",
    "        formatted_metrics = {\n",
    "            \"Model\": model_name,\n",
    "            \"Train Accuracy\": f\"{metrics['Train Accuracy'][0]:.4f} ± {metrics['Train Accuracy'][1]:.4f}\",\n",
    "            \"Test Accuracy\": f\"{metrics['Test Accuracy'][0]:.4f} ± {metrics['Test Accuracy'][1]:.4f}\",\n",
    "            \"F1-Score_1\": f\"{metrics['F1-Score_1'][0]:.4f} ± {metrics['F1-Score_1'][1]:.4f}\",\n",
    "            \"Recall_1\": f\"{metrics['Recall_1'][0]:.4f} ± {metrics['Recall_1'][1]:.4f}\",\n",
    "            \"ROC-AUC_1\": f\"{metrics['ROC-AUC_1'][0]:.4f} ± {metrics['ROC-AUC_1'][1]:.4f}\",\n",
    "        }\n",
    "\n",
    "        results.append(formatted_metrics)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(by=\"ROC-AUC_1\", ascending=False)\n",
    "\n",
    "    print(\"\\nErgebnisse der Modelle:\")\n",
    "    print(results_df)\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Die Datei wurde nicht gefunden. Bitte überprüfen Sie den Pfad.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ein Fehler ist aufgetreten: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee5f56-6524-4d35-ae03-b12a71b567f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Laden der Daten\n",
    "def load_data(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    X = df.drop(columns=[\"Verletzungsstatus\"])\n",
    "    y = df[\"Verletzungsstatus\"]\n",
    "    return X, y\n",
    "\n",
    "# Vorverarbeitung der Daten\n",
    "def preprocess_data(X):\n",
    "    dummy_var = X[\"Geschlecht_weiblich\"]\n",
    "    cols_to_scale = [col for col in X.columns if col != \"Geschlecht_weiblich\"]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[cols_to_scale] = scaler.fit_transform(X[cols_to_scale])\n",
    "    X_scaled[\"Geschlecht_weiblich\"] = dummy_var  \n",
    "    return X_scaled\n",
    "\n",
    "# Berechnung der Feature-Selektion basierend auf Korrelationsmatrix und Mutual Information\n",
    "def calculate_feature_selection(X_train, y_train):\n",
    "    # Berechnungen für Feature Selektion\n",
    "    correlation_matrix = X_train.corr()\n",
    "    mutual_info = mutual_info_classif(X_train, y_train)\n",
    "\n",
    "    # Entferne variablen mit hoher Korrelation (>0.7) und geringerer Mutual Information\n",
    "    selected_features = []\n",
    "    for col in X_train.columns:\n",
    "        if correlation_matrix[col].max() < 0.7 and mutual_info[col] > 0.1:  # Schwellwert anpassen\n",
    "            selected_features.append(col)\n",
    "    return selected_features\n",
    "\n",
    "# Funktion für innere Cross-Validation mit Berechnung der durchschnittlichen Anzahl an Features\n",
    "def inner_cv_feature_selection(X_train, y_train):\n",
    "    inner_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    selected_features_per_fold = []\n",
    "\n",
    "    for inner_train_idx, inner_test_idx in inner_cv.split(X_train, y_train):\n",
    "        X_inner_train, X_inner_test = X_train.iloc[inner_train_idx], X_train.iloc[inner_test_idx]\n",
    "        y_inner_train, y_inner_test = y_train.iloc[inner_train_idx], y_train.iloc[inner_test_idx]\n",
    "        \n",
    "        selected_features = calculate_feature_selection(X_inner_train, y_inner_train)\n",
    "        selected_features_per_fold.append(len(selected_features))  # Anzahl der Features für dieses Fold\n",
    "    \n",
    "    avg_selected_features = np.mean(selected_features_per_fold)\n",
    "    return avg_selected_features  # Durchschnittliche Anzahl an Features\n",
    "\n",
    "# Optuna-Optimierung mit durchschnittlicher Anzahl an Features aus der inneren CV als Zielwert\n",
    "def optimize_feature_selection(X_train, y_train, target_num_features):\n",
    "    def objective(trial):\n",
    "        num_features = trial.suggest_int(\"num_features\", max(5, target_num_features-5), min(50, target_num_features+5))\n",
    "        X_train_optimized = X_train[X_train.columns[:num_features]]  # Beispielhafte Selektion von Features\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        score = cross_val_score(model, X_train_optimized, y_train, cv=5, scoring=\"roc_auc\").mean()\n",
    "        return score\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    return study.best_trial\n",
    "\n",
    "# Funktion für äußere Cross-Validation mit Feature-Optimierung über Optuna\n",
    "def cross_validate(X, y):\n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    for outer_train_idx, outer_test_idx in outer_cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[outer_train_idx], X.iloc[outer_test_idx]\n",
    "        y_train, y_test = y.iloc[outer_train_idx], y.iloc[outer_test_idx]\n",
    "\n",
    "        # Innere Cross-Validation zur Berechnung der durchschnittlichen Anzahl an Features\n",
    "        avg_selected_features = inner_cv_feature_selection(X_train, y_train)\n",
    "\n",
    "        # Optuna-Optimierung in der äußeren CV, basierend auf der durchschnittlichen Anzahl der Features\n",
    "        trial = optimize_feature_selection(X_train, y_train, avg_selected_features)\n",
    "        num_features_optimized = trial.params['num_features']\n",
    "        X_train_optimized = X_train[X_train.columns[:num_features_optimized]]\n",
    "\n",
    "        # Logistische Regression auf den selektierten Features\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train_optimized, y_train)\n",
    "        \n",
    "        # Ergebnisse für den aktuellen Fold der äußeren CV\n",
    "        y_train_pred = model.predict(X_train_optimized)\n",
    "        y_test_pred = model.predict(X_test[X_test.columns[:num_features_optimized]])\n",
    "\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        f1 = f1_score(y_test, y_test_pred)\n",
    "        recall = recall_score(y_test, y_test_pred)\n",
    "        roc_auc = roc_auc_score(y_test, model.predict_proba(X_test[X_test.columns[:num_features_optimized]])[:, 1])\n",
    "\n",
    "        results.append({\n",
    "            'Train Accuracy': train_accuracy,\n",
    "            'Test Accuracy': test_accuracy,\n",
    "            'F1-Score': f1,\n",
    "            'Recall': recall,\n",
    "            'ROC-AUC': roc_auc,\n",
    "            'Num Features': num_features_optimized,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def main(file_path):\n",
    "    X, y = load_data(file_path)\n",
    "    X_scaled = preprocess_data(X)\n",
    "\n",
    "    # Durchführung der äußeren Cross-Validation und Ausgabe der Ergebnisse\n",
    "    results_df = cross_validate(X_scaled, y)\n",
    "    \n",
    "    # Ausgabe der Ergebnisse\n",
    "    print(\"\\nErgebnisse der äußeren Cross-Validation:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Durchschnittliche Konfusionsmatrix über die äußere CV berechnen\n",
    "    confusion_matrix_avg = np.mean([confusion_matrix(y_test, y_test_pred) for y_test, y_test_pred in results_df.iterrows()], axis=0)\n",
    "    print(\"\\nDurchschnittliche Konfusionsmatrix:\")\n",
    "    print(confusion_matrix_avg)\n",
    "\n",
    "    # Visualisierung der Ergebnisse\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=results_df.drop(columns=['Num Features']), orient=\"h\")\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.title(\"Modellvergleich anhand der Evaluierungsmetriken\")\n",
    "    plt.show()\n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\Basistabelle\\Basistabelle_ML.xlsx\"\n",
    "main(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a254b1d8-ab34-40e6-8af2-446fa04c6bb8",
   "metadata": {},
   "source": [
    "test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224e1a7-f8ff-4823-8aca-b6b9a668eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Laden der Daten\n",
    "def load_data(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    X = df.drop(columns=[\"Verletzungsstatus\"])\n",
    "    y = df[\"Verletzungsstatus\"]\n",
    "    return X, y\n",
    "\n",
    "# Vorverarbeitung der Daten\n",
    "def preprocess_data(X):\n",
    "    dummy_var = X[\"Geschlecht_weiblich\"]\n",
    "    cols_to_scale = [col for col in X.columns if col != \"Geschlecht_weiblich\"]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[cols_to_scale] = scaler.fit_transform(X[cols_to_scale])\n",
    "    X_scaled[\"Geschlecht_weiblich\"] = dummy_var  \n",
    "    return X_scaled\n",
    "\n",
    "\n",
    "def calculate_feature_selection(X_train, y_train, correlation_threshold=0.7):\n",
    "    # Berechnungen für Feature Selektion\n",
    "    correlation_matrix = X_train.corr()\n",
    "    mutual_info = mutual_info_classif(X_train, y_train)\n",
    "    \n",
    "    # ausgewählte Features\n",
    "    selected_features = list(X_train.columns)  # Beginne mit allen Features\n",
    "    \n",
    "    # Iteriere über alle Feature-Paare\n",
    "    for col in X_train.columns:\n",
    "        if col in selected_features:\n",
    "            # Suche nach anderen hochkorrelierten Features\n",
    "            correlated_features = correlation_matrix[col].loc[correlation_matrix[col] > correlation_threshold].index.tolist()\n",
    "            correlated_features.remove(col)  # Entferne das aktuelle Feature selbst\n",
    "            \n",
    "            if correlated_features:\n",
    "                # Wenn es hochkorrelierte Features gibt, wähle das mit der höchsten Mutual Information\n",
    "                for correlated_feature in correlated_features:\n",
    "                    if mutual_info[X_train.columns.get_loc(col)] < mutual_info[X_train.columns.get_loc(correlated_feature)]:\n",
    "                        # Entferne das Feature mit der geringeren Mutual Information\n",
    "                        if col in selected_features:\n",
    "                            selected_features.remove(col)\n",
    "                    else:\n",
    "                        # Entferne das Feature mit der geringeren Mutual Information\n",
    "                        if correlated_feature in selected_features:\n",
    "                            selected_features.remove(correlated_feature)\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "\n",
    "# Funktion für innere Cross-Validation mit Berechnung der durchschnittlichen Anzahl an Features\n",
    "def inner_cv_feature_selection(X_train, y_train):\n",
    "    inner_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    selected_features_per_fold = []\n",
    "\n",
    "    for inner_train_idx, inner_test_idx in inner_cv.split(X_train, y_train):\n",
    "        X_inner_train, X_inner_test = X_train.iloc[inner_train_idx], X_train.iloc[inner_test_idx]\n",
    "        y_inner_train, y_inner_test = y_train.iloc[inner_train_idx], y_train.iloc[inner_test_idx]\n",
    "        \n",
    "        selected_features = calculate_feature_selection(X_inner_train, y_inner_train)\n",
    "        selected_features_per_fold.append(len(selected_features))  # Anzahl der Features für dieses Fold\n",
    "    \n",
    "    avg_selected_features = np.mean(selected_features_per_fold)\n",
    "    return avg_selected_features  # Durchschnittliche Anzahl an Features\n",
    "\n",
    "# Optuna-Optimierung mit durchschnittlicher Anzahl an Features aus der inneren CV als Zielwert\n",
    "def optimize_feature_selection(X_train, y_train, target_num_features):\n",
    "    def objective(trial):\n",
    "        num_features = trial.suggest_int(\"num_features\", max(5, target_num_features-5), min(50, target_num_features+5))\n",
    "        X_train_optimized = X_train[X_train.columns[:num_features]]  # Beispielhafte Selektion von Features\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        score = cross_val_score(model, X_train_optimized, y_train, cv=5, scoring=\"roc_auc\").mean()\n",
    "        return score\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    return study.best_trial\n",
    "\n",
    "# Funktion für äußere Cross-Validation mit Feature-Optimierung über Optuna\n",
    "def cross_validate(X, y):\n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    for outer_train_idx, outer_test_idx in outer_cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[outer_train_idx], X.iloc[outer_test_idx]\n",
    "        y_train, y_test = y.iloc[outer_train_idx], y.iloc[outer_test_idx]\n",
    "\n",
    "        # Innere Cross-Validation zur Berechnung der durchschnittlichen Anzahl an Features\n",
    "        avg_selected_features = inner_cv_feature_selection(X_train, y_train)\n",
    "\n",
    "        # Optuna-Optimierung in der äußeren CV, basierend auf der durchschnittlichen Anzahl der Features\n",
    "        trial = optimize_feature_selection(X_train, y_train, avg_selected_features)\n",
    "        num_features_optimized = trial.params['num_features']\n",
    "        X_train_optimized = X_train[X_train.columns[:num_features_optimized]]\n",
    "\n",
    "        # Logistische Regression auf den selektierten Features\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train_optimized, y_train)\n",
    "        \n",
    "        # Ergebnisse für den aktuellen Fold der äußeren CV\n",
    "        y_train_pred = model.predict(X_train_optimized)\n",
    "        y_test_pred = model.predict(X_test[X_test.columns[:num_features_optimized]])\n",
    "\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        f1 = f1_score(y_test, y_test_pred)\n",
    "        recall = recall_score(y_test, y_test_pred)\n",
    "        roc_auc = roc_auc_score(y_test, model.predict_proba(X_test[X_test.columns[:num_features_optimized]])[:, 1])\n",
    "\n",
    "        results.append({\n",
    "            'Train Accuracy': train_accuracy,\n",
    "            'Test Accuracy': test_accuracy,\n",
    "            'F1-Score': f1,\n",
    "            'Recall': recall,\n",
    "            'ROC-AUC': roc_auc,\n",
    "            'Num Features': num_features_optimized,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def main(file_path):\n",
    "    X, y = load_data(file_path)\n",
    "    X_scaled = preprocess_data(X)\n",
    "\n",
    "    # Durchführung der äußeren Cross-Validation und Ausgabe der Ergebnisse\n",
    "    results_df = cross_validate(X_scaled, y)\n",
    "    \n",
    "    # Ausgabe der Ergebnisse\n",
    "    print(\"\\nErgebnisse der äußeren Cross-Validation:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Durchschnittliche Konfusionsmatrix über die äußere CV berechnen\n",
    "    confusion_matrix_avg = np.mean([confusion_matrix(y_test, y_test_pred) for y_test, y_test_pred in results_df.iterrows()], axis=0)\n",
    "    print(\"\\nDurchschnittliche Konfusionsmatrix:\")\n",
    "    print(confusion_matrix_avg)\n",
    "\n",
    "    # Visualisierung der Ergebnisse\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=results_df.drop(columns=['Num Features']), orient=\"h\")\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.title(\"Modellvergleich anhand der Evaluierungsmetriken\")\n",
    "    plt.show()\n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = r\"C:\\Users\\michi\\Documents\\Basistabelle_alle_PostOP_ML.xlsx\"\n",
    "main(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f4aaa-2548-4f33-adb1-0523ed4ec36a",
   "metadata": {},
   "source": [
    "standardabweichungen und konfuionsmatrizen in äußerer CV ergänzen!! zudem die features ausgeben lassen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c05ec4-24f4-429f-a245-916b0b77941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Laden der Daten\n",
    "def load_data(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    X = df.drop(columns=[\"Verletzungsstatus\"])\n",
    "    y = df[\"Verletzungsstatus\"]\n",
    "    return X, y\n",
    "\n",
    "# Vorverarbeitung der Daten\n",
    "def preprocess_data(X):\n",
    "    dummy_var = X[\"Geschlecht_weiblich\"]\n",
    "    cols_to_scale = [col for col in X.columns if col != \"Geschlecht_weiblich\"]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[cols_to_scale] = scaler.fit_transform(X[cols_to_scale])\n",
    "    X_scaled[\"Geschlecht_weiblich\"] = dummy_var  \n",
    "    return X_scaled\n",
    "\n",
    "# Auswahl an nicht hochkorrelierten Features herstellen\n",
    "def calculate_feature_selection(X_train, y_train, correlation_threshold=0.7):\n",
    "    # Berechnungen für Feature Selektion\n",
    "    correlation_matrix = X_train.corr()\n",
    "    mutual_info = mutual_info_classif(X_train, y_train)\n",
    "    \n",
    "    # ausgewählte Features\n",
    "    selected_features = list(X_train.columns)  # Beginne mit allen Features\n",
    "    \n",
    "    # Iteriere über alle Feature-Paare\n",
    "    for col in X_train.columns:\n",
    "        if col in selected_features:\n",
    "            # Suche nach anderen hochkorrelierten Features\n",
    "            correlated_features = correlation_matrix[col].loc[correlation_matrix[col] > correlation_threshold].index.tolist()\n",
    "            correlated_features.remove(col)  # Entferne das aktuelle Feature selbst\n",
    "            \n",
    "            if correlated_features:\n",
    "                # Wenn es hochkorrelierte Features gibt, wähle das mit der höchsten Mutual Information\n",
    "                for correlated_feature in correlated_features:\n",
    "                    if mutual_info[X_train.columns.get_loc(col)] < mutual_info[X_train.columns.get_loc(correlated_feature)]:\n",
    "                        # Entferne das Feature mit der geringeren Mutual Information\n",
    "                        if col in selected_features:\n",
    "                            selected_features.remove(col)\n",
    "                    else:\n",
    "                        # Entferne das Feature mit der geringeren Mutual Information\n",
    "                        if correlated_feature in selected_features:\n",
    "                            selected_features.remove(correlated_feature)\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "\n",
    "# Optuna-Optimierung für äußere CV mit durchschnittlicher Anzahl an Features aus der inneren CV als Zielwert\n",
    "def optimize_feature_selection(X_train, y_train, target_num_features):\n",
    "    # Zuerst die nicht hochkorrelierten Features bestimmen\n",
    "    candidate_features = calculate_feature_selection(X_train, y_train, correlation_threshold=0.7)\n",
    "    # Stelle sicher, dass target_num_features nicht größer als die Anzahl der Kandidaten ist:\n",
    "    target_num_features = min(target_num_features, len(candidate_features))\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Der Suchraum orientiert sich an target_num_features, aber es darf nicht mehr als die\n",
    "        # vorhandenen Kandidatenfeatures ausgewählt werden.\n",
    "        num_features = trial.suggest_int(\n",
    "            \"num_features\",\n",
    "            max(5, target_num_features-5),\n",
    "            min(len(candidate_features), target_num_features+5)\n",
    "        )\n",
    "        \n",
    "        # Falls num_features größer als die Anzahl der Kandidatenfeatures sein könnte, beschränke sie\n",
    "        num_features = min(num_features, len(candidate_features))\n",
    "        \n",
    "        # Auswahl der ersten num_features aus dem Kandidatenpool\n",
    "        X_train_optimized = X_train[candidate_features[:num_features]]\n",
    "        \n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        score = cross_val_score(model, X_train_optimized, y_train, cv=5, scoring=\"roc_auc\").mean()\n",
    "        return score\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    return study.best_trial\n",
    "\n",
    "\n",
    "# komplette innere CV mit Berechnung der duchschnittlichen Anzahl an Features als Ausgabe\n",
    "def inner_cv_feature_selection(X_train, y_train):\n",
    "    inner_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    best_num_features = []\n",
    "\n",
    "    for inner_train_idx, inner_test_idx in inner_cv.split(X_train, y_train):\n",
    "        X_inner_train, X_inner_test = X_train.iloc[inner_train_idx], X_train.iloc[inner_test_idx]\n",
    "        y_inner_train, y_inner_test = y_train.iloc[inner_train_idx], y_train.iloc[inner_test_idx]\n",
    "\n",
    "        # Features ohne hohe Korrelation zur Auswahl für Optuna\n",
    "        candidate_features = calculate_feature_selection(X_inner_train, y_inner_train, correlation_threshold=0.7)\n",
    "\n",
    "        # Achte darauf, dass der Kandidatenpool nicht leer ist und mindestens eine Feature enthält:\n",
    "        if len(candidate_features) == 0:\n",
    "            candidate_features = list(X_inner_train.columns)\n",
    "    \n",
    "        # Optuna für jedes innere Fold\n",
    "        def objective(trial):\n",
    "            num_features = trial.suggest_int(\"num_features\", 5, 50)\n",
    "            selected_features = X_inner_train.columns[:num_features]\n",
    "            X_selected = X_inner_train[selected_features]\n",
    "\n",
    "            model = LogisticRegression(max_iter=1000)\n",
    "            score = cross_val_score(model, X_selected, y_inner_train, cv=5, scoring=\"roc_auc\").mean()\n",
    "            return score\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=100)\n",
    "        \n",
    "        best_num_features.append(study.best_trial.params[\"num_features\"])  # Speichere bestes Feature-Set\n",
    "        \n",
    "    return int(np.mean(best_num_features)) # Durchschnittliche Feature-Anzahl \n",
    "\n",
    "\n",
    "# Funktion für äußere Cross-Validation mit Feature-Optimierung über Optuna\n",
    "def cross_validate(X, y):\n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    for outer_train_idx, outer_test_idx in outer_cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[outer_train_idx], X.iloc[outer_test_idx]\n",
    "        y_train, y_test = y.iloc[outer_train_idx], y.iloc[outer_test_idx]\n",
    "\n",
    "\n",
    "        # Innere Cross-Validation zur Berechnung der durchschnittlichen Anzahl an Features\n",
    "        avg_selected_features = inner_cv_feature_selection(X_train, y_train)\n",
    "\n",
    "        # Optuna-Optimierung in der äußeren CV, basierend auf der durchschnittlichen Anzahl der Features\n",
    "        trial = optimize_feature_selection(X_train, y_train, avg_selected_features)\n",
    "        num_features_optimized = trial.params['num_features']\n",
    "        X_train_optimized = X_train[X_train.columns[:num_features_optimized]]\n",
    "\n",
    "        # Logistische Regression auf den selektierten Features\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train_optimized, y_train)\n",
    "        \n",
    "        # Ergebnisse für den aktuellen Fold der äußeren CV\n",
    "        y_train_pred = model.predict(X_train_optimized)\n",
    "        y_test_pred = model.predict(X_test[X_test.columns[:num_features_optimized]])\n",
    "\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        f1 = f1_score(y_test, y_test_pred)\n",
    "        recall = recall_score(y_test, y_test_pred)\n",
    "        roc_auc = roc_auc_score(y_test, model.predict_proba(X_test[X_test.columns[:num_features_optimized]])[:, 1])\n",
    "\n",
    "        results.append({\n",
    "            'Train Accuracy': train_accuracy,\n",
    "            'Test Accuracy': test_accuracy,\n",
    "            'F1-Score': f1,\n",
    "            'Recall': recall,\n",
    "            'ROC-AUC': roc_auc,\n",
    "            'Num Features': num_features_optimized,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def main(file_path):\n",
    "    X, y = load_data(file_path)\n",
    "    X_scaled = preprocess_data(X)\n",
    "\n",
    "    # Durchführung der äußeren Cross-Validation und Ausgabe der Ergebnisse\n",
    "    results_df = cross_validate(X_scaled, y)\n",
    "    \n",
    "    # Ausgabe der Ergebnisse\n",
    "    print(\"\\nErgebnisse der äußeren Cross-Validation:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Visualisierung der Ergebnisse\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=results_df.drop(columns=['Num Features']), orient=\"h\")\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.title(\"Modellvergleich anhand der Evaluierungsmetriken\")\n",
    "    plt.show()\n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = r\"C:\\Users\\michi\\Documents\\Basistabelle_alle_PostOP_ML.xlsx\"\n",
    "main(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6faec5-e9cd-4757-bc18-137235122b07",
   "metadata": {},
   "source": [
    "sollte grundsätzlich funktionieren, testen!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c7456-bb1b-4bb8-9108-d2ba9a7dc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Laden der Daten\n",
    "def load_data(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    X = df.drop(columns=[\"Verletzungsstatus\"])\n",
    "    y = df[\"Verletzungsstatus\"]\n",
    "    return X, y\n",
    "\n",
    "# Vorverarbeitung der Daten\n",
    "def preprocess_data(X):\n",
    "    dummy_var = X[\"Geschlecht_weiblich\"]\n",
    "    cols_to_scale = [col for col in X.columns if col != \"Geschlecht_weiblich\"]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[cols_to_scale] = scaler.fit_transform(X[cols_to_scale])\n",
    "    X_scaled[\"Geschlecht_weiblich\"] = dummy_var  \n",
    "    return X_scaled\n",
    "\n",
    "# Auswahl an nicht hochkorrelierten Features herstellen\n",
    "def calculate_feature_selection(X_train, y_train, correlation_threshold=0.7):\n",
    "    # Berechnungen für Feature Selektion\n",
    "    correlation_matrix = X_train.corr()\n",
    "    mutual_info = mutual_info_classif(X_train, y_train)\n",
    "    \n",
    "    # ausgewählte Features\n",
    "    selected_features = list(X_train.columns)  # Beginne mit allen Features\n",
    "    \n",
    "    # Iteriere über alle Feature-Paare\n",
    "    for col in X_train.columns:\n",
    "        if col in selected_features:\n",
    "            # Suche nach anderen hochkorrelierten Features\n",
    "            correlated_features = correlation_matrix[col].loc[correlation_matrix[col] > correlation_threshold].index.tolist()\n",
    "            correlated_features.remove(col)  # Entferne das aktuelle Feature selbst\n",
    "            \n",
    "            if correlated_features:\n",
    "                # Wenn es hochkorrelierte Features gibt, wähle das mit der höchsten Mutual Information\n",
    "                for correlated_feature in correlated_features:\n",
    "                    if mutual_info[X_train.columns.get_loc(col)] < mutual_info[X_train.columns.get_loc(correlated_feature)]:\n",
    "                        # Entferne das Feature mit der geringeren Mutual Information\n",
    "                        if col in selected_features:\n",
    "                            selected_features.remove(col)\n",
    "                    else:\n",
    "                        # Entferne das Feature mit der geringeren Mutual Information\n",
    "                        if correlated_feature in selected_features:\n",
    "                            selected_features.remove(correlated_feature)\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "\n",
    "# Optuna-Optimierung für äußere CV mit durchschnittlicher Anzahl an Features aus der inneren CV als Zielwert\n",
    "def optimize_feature_selection(X_train, y_train, target_num_features):\n",
    "    # Zuerst die nicht hochkorrelierten Features bestimmen\n",
    "    candidate_features = calculate_feature_selection(X_train, y_train, correlation_threshold=0.7)\n",
    "    # Stelle sicher, dass target_num_features nicht größer als die Anzahl der Kandidaten ist:\n",
    "    target_num_features = min(target_num_features, len(candidate_features))\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Der Suchraum orientiert sich an target_num_features, aber es darf nicht mehr als die\n",
    "        # vorhandenen Kandidatenfeatures ausgewählt werden.\n",
    "        num_features = trial.suggest_int(\n",
    "            \"num_features\",\n",
    "            max(5, target_num_features-5),\n",
    "            min(len(candidate_features), target_num_features+5)\n",
    "        )\n",
    "        \n",
    "        # Falls num_features größer als die Anzahl der Kandidatenfeatures sein könnte, beschränke sie\n",
    "        num_features = min(num_features, len(candidate_features))\n",
    "        \n",
    "        # Auswahl der ersten num_features aus dem Kandidatenpool\n",
    "        X_train_optimized = X_train[candidate_features[:num_features]]\n",
    "        \n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        score = cross_val_score(model, X_train_optimized, y_train, cv=5, scoring=\"roc_auc\").mean()\n",
    "        return score\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    return study.best_trial\n",
    "\n",
    "\n",
    "# komplette innere CV mit Berechnung der duchschnittlichen Anzahl an Features als Ausgabe\n",
    "def inner_cv_feature_selection(X_train, y_train):\n",
    "    inner_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    best_num_features = []\n",
    "\n",
    "    for inner_train_idx, inner_test_idx in inner_cv.split(X_train, y_train):\n",
    "        X_inner_train, X_inner_test = X_train.iloc[inner_train_idx], X_train.iloc[inner_test_idx]\n",
    "        y_inner_train, y_inner_test = y_train.iloc[inner_train_idx], y_train.iloc[inner_test_idx]\n",
    "\n",
    "        # Berechne den Kandidatenpool\n",
    "        candidate_features = calculate_feature_selection(X_inner_train, y_inner_train, correlation_threshold=0.7)\n",
    "        \n",
    "        # Achte darauf, dass der Kandidatenpool nicht leer ist und mindestens eine Feature enthält:\n",
    "        if len(candidate_features) == 0:\n",
    "            candidate_features = list(X_inner_train.columns)\n",
    "        \n",
    "        def objective(trial):\n",
    "            # Setze die Grenzen im Suchraum basierend auf der Länge des Kandidatenpools\n",
    "            low_bound = 1 \n",
    "            high_bound = len(candidate_features)\n",
    "            num_features = trial.suggest_int(\"num_features\", low_bound, high_bound)\n",
    "            X_selected = X_inner_train[candidate_features[:num_features]]\n",
    "\n",
    "            model = LogisticRegression(max_iter=1000)\n",
    "            score = cross_val_score(model, X_selected, y_inner_train, cv=5, scoring=\"roc_auc\").mean()\n",
    "            return score\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=100)\n",
    "        best_num_features.append(study.best_trial.params[\"num_features\"])\n",
    "        \n",
    "    return int(np.mean(best_num_features))\n",
    "\n",
    "# Funktion für äußere Cross-Validation mit Feature-Optimierung über Optuna\n",
    "def cross_validate(X, y):\n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    for outer_train_idx, outer_test_idx in outer_cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[outer_train_idx], X.iloc[outer_test_idx]\n",
    "        y_train, y_test = y.iloc[outer_train_idx], y.iloc[outer_test_idx]\n",
    "\n",
    "\n",
    "        # Innere Cross-Validation zur Berechnung der durchschnittlichen Anzahl an Features\n",
    "        avg_selected_features = inner_cv_feature_selection(X_train, y_train)\n",
    "\n",
    "        # Optuna-Optimierung in der äußeren CV, basierend auf der durchschnittlichen Anzahl der Features\n",
    "        candidate_features = calculate_feature_selection(X_train, y_train, correlation_threshold=0.7)\n",
    "        trial = optimize_feature_selection(X_train[candidate_features], y_train, avg_selected_features)\n",
    "        num_features_optimized = trial.params['num_features']\n",
    "        X_train_optimized = X_train[candidate_features[:num_features_optimized]]\n",
    "        X_test_optimized = X_test[candidate_features[:num_features_optimized]]\n",
    "\n",
    "        # Logistische Regression auf den selektierten Features\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train_optimized, y_train)\n",
    "        \n",
    "        # Ergebnisse für den aktuellen Fold der äußeren CV\n",
    "        y_train_pred = model.predict(X_train_optimized)\n",
    "        y_test_pred = model.predict(X_test[X_test.columns[:num_features_optimized]])\n",
    "\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        f1 = f1_score(y_test, y_test_pred)\n",
    "        recall = recall_score(y_test, y_test_pred)\n",
    "        roc_auc = roc_auc_score(y_test, model.predict_proba(X_test[X_test.columns[:num_features_optimized]])[:, 1])\n",
    "\n",
    "        results.append({\n",
    "            'Train Accuracy': train_accuracy,\n",
    "            'Test Accuracy': test_accuracy,\n",
    "            'F1-Score': f1,\n",
    "            'Recall': recall,\n",
    "            'ROC-AUC': roc_auc,\n",
    "            'Num Features': num_features_optimized,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def main(file_path):\n",
    "    X, y = load_data(file_path)\n",
    "    X_scaled = preprocess_data(X)\n",
    "\n",
    "    # Durchführung der äußeren Cross-Validation und Ausgabe der Ergebnisse\n",
    "    results_df = cross_validate(X_scaled, y)\n",
    "    \n",
    "    # Ausgabe der Ergebnisse\n",
    "    print(\"\\nErgebnisse der äußeren Cross-Validation:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Visualisierung der Ergebnisse\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=results_df.drop(columns=['Num Features']), orient=\"h\")\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.title(\"Modellvergleich anhand der Evaluierungsmetriken\")\n",
    "    plt.show()\n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = r\"C:\\Users\\michi\\Documents\\Basistabelle_alle_PostOP_ML.xlsx\"\n",
    "main(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4698583-b509-49f2-aa1d-29ee6dd1a51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   FS_70_feature_importance.ipynb\n",
      "\tmodified:   Feature_Selektion_ML.ipynb\n",
      "\tmodified:   ML_Motum.ipynb\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t.OpenSim/\n",
      "\t.anaconda/\n",
      "\t.conda/\n",
      "\t.condarc\n",
      "\t.continuum/\n",
      "\t.gitconfig\n",
      "\t.ipynb_checkpoints/\n",
      "\t.ipython/\n",
      "\t.jline-jython.history\n",
      "\t.jupyter/\n",
      "\t.keras/\n",
      "\t.matplotlib/\n",
      "\t.pdfbox.cache\n",
      "\tAppData/\n",
      "\tContacts/\n",
      "\tDocuments/\n",
      "\tDownloads/\n",
      "\tFavorites/\n",
      "\tLinks/\n",
      "\tML_jupyter_notebook/\n",
      "\tMusic/\n",
      "\tNTUSER.DAT\n",
      "\tNTUSER.DAT{dcce88e9-6a04-11ed-b44a-988eb0a321d6}.TM.blf\n",
      "\tNTUSER.DAT{dcce88e9-6a04-11ed-b44a-988eb0a321d6}.TMContainer00000000000000000001.regtrans-ms\n",
      "\tNTUSER.DAT{dcce88e9-6a04-11ed-b44a-988eb0a321d6}.TMContainer00000000000000000002.regtrans-ms\n",
      "\tOneDrive/\n",
      "\tPycharmProjects/\n",
      "\tSaved Games/\n",
      "\tSearches/\n",
      "\tTEST_tpot.ipynb\n",
      "\tTest_ML.ipynb\n",
      "\tUntitled.ipynb\n",
      "\tUntitled1.ipynb\n",
      "\tUntitled2.ipynb\n",
      "\tUntitled3.ipynb\n",
      "\tUntitled4.ipynb\n",
      "\tUntitled5.ipynb\n",
      "\tVideos/\n",
      "\ta8d141d5a09f7f6f1c06174f51a31262/\n",
      "\tntuser.dat.LOG1\n",
      "\tntuser.dat.LOG2\n",
      "\tntuser.ini\n",
      "\t\"\\303\\234bung_polynomiale_Regression.ipynb\"\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: could not open directory 'Anwendungsdaten/': Permission denied\n",
      "warning: could not open directory 'Cookies/': Permission denied\n",
      "warning: could not open directory 'Druckumgebung/': Permission denied\n",
      "warning: could not open directory 'Eigene Dateien/': Permission denied\n",
      "warning: could not open directory 'Lokale Einstellungen/': Permission denied\n",
      "warning: could not open directory 'Netzwerkumgebung/': Permission denied\n",
      "warning: could not open directory 'Recent/': Permission denied\n",
      "warning: could not open directory 'SendTo/': Permission denied\n",
      "warning: could not open directory 'StartmenÃ¼/': Permission denied\n",
      "warning: could not open directory 'Vorlagen/': Permission denied\n"
     ]
    }
   ],
   "source": [
    "! git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147c841-eec0-4280-8e1f-04a9ebb5bf9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
