{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed933f4-092f-41ef-b796-a4caff3adc38",
   "metadata": {},
   "source": [
    "Daten einlesen, Zielvariable festlegen und X-Daten normalisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def03203-00a5-431d-a6c1-8fee43acf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\ml_algorithmus_finden.xlsx\"\n",
    "\n",
    "# Excel-Datei einlesen\n",
    "try:\n",
    "    # Direkt die Datei einlesen (erstes Tabellenblatt standardmäßig)\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Informationen anzeigen\n",
    "    print(\"\\nErste Zeilen der Tabelle:\")\n",
    "    print(df.head())  # Zeigt die ersten 5 Zeilen an\n",
    "\n",
    "    print(\"\\nInformationen über die Tabelle:\")\n",
    "    print(df.info())  # Struktur der Tabelle\n",
    "\n",
    "    # Zielvariable (y) und Features (X) extrahieren\n",
    "    y = df['Verletzungsstatus']\n",
    "    X = df.drop(columns=['Verletzungsstatus'])  # Entferne die Zielvariable aus den Features\n",
    "\n",
    "    # Skalierung der Features (X) mit StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)  # Normiert X auf den Standardbereich (Mittelwert = 0, Std = 1)\n",
    "\n",
    "    # Ausgabe nach Skalierung (optional)\n",
    "    print(\"\\nBeispiel nach Skalierung (erste Zeilen):\")\n",
    "    print(pd.DataFrame(X_scaled, columns=X.columns).head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Die Datei wurde nicht gefunden. Bitte überprüfen Sie den Pfad.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ein Fehler ist aufgetreten: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ff075-4eef-44d3-b75c-9c752e70ee71",
   "metadata": {},
   "source": [
    "Modelle vergleichen mit Standardeinstellungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bf2b1-446a-4291-9136-bafa0fc4a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def repeated_k_fold(model, X, y, n_splits=5, n_repeats=10):\n",
    "    \"\"\"Führt repeated k-fold cross-validation durch und berechnet die Metriken.\"\"\"\n",
    "    # Repeated Stratified K-Fold\n",
    "    rkf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
    "\n",
    "    # Metriken definieren\n",
    "    accuracy_train = []\n",
    "    accuracy_test = []\n",
    "    f1 = []\n",
    "    roc_auc = []\n",
    "\n",
    "    for train_index, test_index in rkf.split(X, y):\n",
    "        # Splitte die Daten in Trainings- und Testdaten\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Trainiere das Modell\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Vorhersagen für Trainings- und Testdaten\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Berechne die Metriken\n",
    "        accuracy_train.append(accuracy_score(y_train, y_train_pred))\n",
    "        accuracy_test.append(accuracy_score(y_test, y_test_pred))\n",
    "        f1.append(f1_score(y_test, y_test_pred))\n",
    "        roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "    # Durchschnittswerte berechnen\n",
    "    return {\n",
    "        \"Train Accuracy\": np.mean(accuracy_train),\n",
    "        \"Test Accuracy\": np.mean(accuracy_test),\n",
    "        \"F1-Score\": np.mean(f1),\n",
    "        \"ROC-AUC\": np.mean(roc_auc),\n",
    "    }\n",
    "\n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\ml_algorithmus_finden.xlsx\"\n",
    "\n",
    "# Excel-Datei einlesen\n",
    "try:\n",
    "    # Direkt die Datei einlesen (erstes Tabellenblatt standardmäßig)\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Zielvariable (y) und Features (X) extrahieren\n",
    "    y = df['Verletzungsstatus']\n",
    "    X = df.drop(columns=['Verletzungsstatus'])  # Entferne die Zielvariable aus den Features\n",
    "\n",
    "    # Skalierung der Features (X) mit StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)  # Normiert X auf den Standardbereich (Mittelwert = 0, Std = 1)\n",
    "\n",
    "    # Modelle definieren\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42),\n",
    "        \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "        \"SVC\": SVC(probability=True, random_state=42),\n",
    "        \"k-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "        \"MLP Classifier\": MLPClassifier(max_iter=1000, random_state=42),\n",
    "        \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "        \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(),\n",
    "        \"Quadratic Discriminant Analysis\": QuadraticDiscriminantAnalysis(),\n",
    "        \"Bagging Classifier\": BaggingClassifier(random_state=42),\n",
    "        \"Extra Trees\": ExtraTreesClassifier(random_state=42),\n",
    "    }\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    results = []\n",
    "\n",
    "    # Validierung jedes Modells\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Modell wird validiert: {model_name}\")\n",
    "        metrics = repeated_k_fold(model, X_scaled, y)  # Verwende X_scaled statt X\n",
    "        results.append({\"Model\": model_name, **metrics})\n",
    "\n",
    "    # Ergebnisse in DataFrame konvertieren und sortieren\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(by=\"ROC-AUC\", ascending=False)\n",
    "\n",
    "    # Ergebnisse anzeigen\n",
    "    print(\"\\nErgebnisse der Modelle:\")\n",
    "    print(results_df)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Die Datei wurde nicht gefunden. Bitte überprüfen Sie den Pfad.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ein Fehler ist aufgetreten: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2f149-1092-4d73-8353-58d4acda9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e17296a-cb80-46f1-8203-137be8cd83bf",
   "metadata": {},
   "source": [
    "Auto ML mit TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80365055-92f5-4e7e-9d44-8122e2ef858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Daten einlesen\n",
    "file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\ml_algorithmus_finden.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Zielvariable (y) und Features (X) extrahieren\n",
    "y = df['Verletzungsstatus']\n",
    "X = df.drop(columns=['Verletzungsstatus'])\n",
    "\n",
    "# Skalierung der Features (X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Trainings- und Testdaten splitten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TPOTClassifier erstellen und anpassen\n",
    "tpot = TPOTClassifier(verbosity=2, generations=5, population_size=20, random_state=42)\n",
    "\n",
    "# Modell mit dem Trainingsdatensatz optimieren\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Beste Lösung ausgeben\n",
    "print(\"Beste Lösung:\", tpot.fitted_pipeline_)\n",
    "\n",
    "# Evaluation des besten Modells auf den Testdaten\n",
    "print(f\"Test Accuracy: {tpot.score(X_test, y_test)}\")\n",
    "\n",
    "# Optional: Export des besten Modells als Python-Code\n",
    "tpot.export('best_model.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a814cad-fd23-4a57-a987-0493793e958d",
   "metadata": {},
   "source": [
    "Einzelnes Modell validieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e8a42-31ea-48ca-8b7d-c625d7997317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def repeated_k_fold(model, X, y, n_splits=5, n_repeats=10):\n",
    "    \"\"\"Führt repeated k-fold cross-validation durch und berechnet die Metriken.\"\"\"\n",
    "    # Repeated Stratified K-Fold\n",
    "    rkf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
    "\n",
    "    # Metriken definieren\n",
    "    accuracy_train = []\n",
    "    accuracy_test = []\n",
    "    f1 = []\n",
    "    roc_auc = []\n",
    "\n",
    "    for train_index, test_index in rkf.split(X, y):\n",
    "        # Splitte die Daten in Trainings- und Testdaten\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Trainiere das Modell\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Vorhersagen für Trainings- und Testdaten\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Berechne die Metriken\n",
    "        accuracy_train.append(accuracy_score(y_train, y_train_pred))\n",
    "        accuracy_test.append(accuracy_score(y_test, y_test_pred))\n",
    "        f1.append(f1_score(y_test, y_test_pred))\n",
    "        roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "    # Durchschnittswerte berechnen\n",
    "    return {\n",
    "        \"Train Accuracy\": np.mean(accuracy_train),\n",
    "        \"Test Accuracy\": np.mean(accuracy_test),\n",
    "        \"F1-Score\": np.mean(f1),\n",
    "        \"ROC-AUC\": np.mean(roc_auc),\n",
    "    }\n",
    "\n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\ml_algorithmus_finden.xlsx\"\n",
    "\n",
    "# Excel-Datei einlesen\n",
    "try:\n",
    "    # Direkt die Datei einlesen (erstes Tabellenblatt standardmäßig)\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Zielvariable (y) und Features (X) extrahieren\n",
    "    y = df['Verletzungsstatus']\n",
    "    X = df.drop(columns=['Verletzungsstatus'])  # Entferne die Zielvariable aus den Features\n",
    "\n",
    "    # Skalierung der Features (X) mit StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)  # Normiert X auf den Standardbereich (Mittelwert = 0, Std = 1)\n",
    "\n",
    "    # Modelle definieren\n",
    "    models = {\n",
    "    \"Extra Trees\": ExtraTreesClassifier(\n",
    "        bootstrap=False, \n",
    "        criterion=\"gini\", \n",
    "        max_features=0.3, \n",
    "        min_samples_leaf=4, \n",
    "        min_samples_split=9, \n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    ),}\n",
    "\n",
    "\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    results = []\n",
    "\n",
    "    # Validierung jedes Modells\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Modell wird validiert: {model_name}\")\n",
    "        metrics = repeated_k_fold(model, X_scaled, y)  # Verwende X_scaled statt X\n",
    "        results.append({\"Model\": model_name, **metrics})\n",
    "\n",
    "    # Ergebnisse in DataFrame konvertieren und sortieren\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(by=\"ROC-AUC\", ascending=False)\n",
    "\n",
    "    # Ergebnisse anzeigen\n",
    "    print(\"\\nErgebnisse der Modelle:\")\n",
    "    print(results_df)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Die Datei wurde nicht gefunden. Bitte überprüfen Sie den Pfad.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ein Fehler ist aufgetreten: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e6739-e7f5-43fa-840f-f2f498f7b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b954e-4f7c-427d-a97e-0bd670938755",
   "metadata": {},
   "source": [
    "Grid Search für ein ausgewähltes Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b13b7-72c5-412b-9066-83e5627aec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "    \n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\ml_algorithmus_finden.xlsx\"\n",
    "\n",
    "# Excel-Datei einlesen\n",
    "try:\n",
    "    # Direkt die Datei einlesen (erstes Tabellenblatt standardmäßig)\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Zielvariable (y) und Features (X) extrahieren\n",
    "    y = df['Verletzungsstatus']\n",
    "    X = df.drop(columns=['Verletzungsstatus'])  # Entferne die Zielvariable aus den Features\n",
    "\n",
    "    # Skalierung der Features (X) mit StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)  # Normiert X auf den Standardbereich (Mittelwert = 0, Std = 1)\n",
    "\n",
    "    # Ausgabe nach Skalierung (optional)\n",
    "    print(\"\\nBeispiel nach Skalierung (erste Zeilen):\")\n",
    "    print(pd.DataFrame(X_scaled, columns=X.columns).head())\n",
    "    \n",
    "    # Train-Test-Split durchführen (80% Training, 20% Test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Modell initialisieren\n",
    "    model = ExtraTreesClassifier(random_state=42)\n",
    "    \n",
    "    # Parameterbereich für GridSearch definieren\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200, 300, 500],\n",
    "        'max_features': ['auto', 'sqrt', 'log2', 0.1, 0.3],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 10, 20, 30, 40],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 5, 10],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "    \n",
    "    # GridSearchCV initialisieren\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                               cv=5, n_jobs=-1, scoring='roc_auc', verbose=1)\n",
    "    \n",
    "    # Führe GridSearchCV aus auf dem Trainingsdatensatz\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Beste Parameter und beste Leistung anzeigen\n",
    "    print(f\"Beste Parameter: {grid_search.best_params_}\")\n",
    "    print(f\"Beste ROC-AUC: {grid_search.best_score_}\")\n",
    "    \n",
    "    # Optional: Testdaten mit dem besten Modell evaluieren\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Vorhersage und Bewertung auf dem Testdatensatz\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    y_test_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_test_prob)\n",
    "    \n",
    "    print(f\"ROC-AUC auf den Testdaten: {roc_auc}\")\n",
    "    \n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Die Datei wurde nicht gefunden. Bitte überprüfen Sie den Pfad.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ein Fehler ist aufgetreten: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa827945-7c72-4254-89cd-3a7adffb31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\ml_algorithmus_finden.xlsx\"\n",
    "\n",
    "# Excel-Datei einlesen\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Zielvariable (y) und Features (X) extrahieren\n",
    "y = df['Verletzungsstatus']\n",
    "X = df.drop(columns=['Verletzungsstatus'])  # Entferne die Zielvariable aus den Features\n",
    "\n",
    "# Skalierung der Features (X) mit StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Normiert X auf den Standardbereich (Mittelwert = 0, Std = 1)\n",
    "\n",
    "# Repeated Stratified K-Fold Cross-Validation\n",
    "rkf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "# Feature Importance speichern\n",
    "feature_importances_list = []\n",
    "\n",
    "# K-Fold Cross-Validation durchführen\n",
    "for train_index, test_index in rkf.split(X_scaled, y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Modell mit den spezifischen Hyperparametern initialisieren und trainieren\n",
    "    model = ExtraTreesClassifier(\n",
    "        bootstrap=True, \n",
    "        criterion=\"entropy\", \n",
    "        max_depth=None, \n",
    "        max_features=0.3, \n",
    "        min_samples_leaf=1, \n",
    "        min_samples_split=5, \n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Feature Importance für den aktuellen Fold speichern\n",
    "    feature_importances_list.append(model.feature_importances_)\n",
    "\n",
    "# Durchschnitt der Feature-Importances über alle Folds berechnen\n",
    "average_importances = np.mean(feature_importances_list, axis=0)\n",
    "\n",
    "# Feature Importance mit den Feature-Namen anzeigen\n",
    "indices = average_importances.argsort()[::-1]  # Absteigend sortieren\n",
    "print(\"Feature Importance (durchschnittlich über alle Folds):\")\n",
    "for i in indices:\n",
    "    print(f\"Feature {X.columns[i]}: {average_importances[i]:.4f}\")\n",
    "\n",
    "# Visualisierung: Balkendiagramm erstellen\n",
    "plt.figure(figsize=(14, 30))  # Größe des Diagramms erhöhen\n",
    "\n",
    "# Balkenbreite und Abstand anpassen\n",
    "sns.barplot(x=average_importances[indices], y=X.columns[indices], orient='h', palette='viridis', width=0.8, hue = None)\n",
    "\n",
    "# Titel und Achsenbeschriftungen anpassen\n",
    "plt.title('Feature Importance (Durchschnitt über alle Folds)', fontsize=16)\n",
    "plt.xlabel('Wichtigkeit', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "\n",
    "# Schriftgröße der Achsenbeschriftungen anpassen\n",
    "plt.xticks(fontsize=12)  # Schriftgröße für x-Achse\n",
    "plt.yticks(fontsize=12)  # Schriftgröße für y-Achse\n",
    "\n",
    "# Diagramm anzeigen\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eebe0c0b-6288-41c1-901c-d202042d2e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is ahead of 'origin/ml_vorlage' by 1 commit.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   ML_Vorlage.ipynb\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t.ipynb_checkpoints/\n",
      "\tDatenexport_Skript_2.0.ipynb\n",
      "\tExtra_Tree_optimieren.ipynb\n",
      "\tExzentrik_finale_Version.ipynb\n",
      "\tHyperparameter.ipynb\n",
      "\tIsokinetik_finale_Version.ipynb\n",
      "\tIsometrie_finale_Version.ipynb\n",
      "\tPCA.ipynb\n",
      "\tPCA_TSA.ipynb\n",
      "\tSprung_Excel_Merge.ipynb\n",
      "\tTSA_grafik.ipynb\n",
      "\tTest.ipynb\n",
      "\tUntitled.ipynb\n",
      "\tUntitled1.ipynb\n",
      "\tgit_hub.ipynb\n",
      "\tgradient_boosting_optimieren.ipynb\n",
      "\tmaestroni_faktoren.ipynb\n",
      "\tmaestroni_statistik_deskriptiv.ipynb\n",
      "\tml_algorithmus_finden.ipynb\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5e510-2f3d-4d19-9d6c-d3224bfccbca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
