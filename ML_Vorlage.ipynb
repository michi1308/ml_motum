{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed933f4-092f-41ef-b796-a4caff3adc38",
   "metadata": {},
   "source": [
    "Daten einlesen, Zielvariable festlegen und X-Daten normalisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def03203-00a5-431d-a6c1-8fee43acf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\ml_algorithmus_finden.xlsx\"\n",
    "\n",
    "# Excel-Datei einlesen\n",
    "try:\n",
    "    # Direkt die Datei einlesen (erstes Tabellenblatt standardmäßig)\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Informationen anzeigen\n",
    "    print(\"\\nErste Zeilen der Tabelle:\")\n",
    "    print(df.head())  # Zeigt die ersten 5 Zeilen an\n",
    "\n",
    "    print(\"\\nInformationen über die Tabelle:\")\n",
    "    print(df.info())  # Struktur der Tabelle\n",
    "\n",
    "    # Zielvariable (y) und Features (X) extrahieren\n",
    "    y = df['Verletzungsstatus']\n",
    "    X = df.drop(columns=['Verletzungsstatus'])  # Entferne die Zielvariable aus den Features\n",
    "\n",
    "    # Skalierung der Features (X) mit StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)  # Normiert X auf den Standardbereich (Mittelwert = 0, Std = 1)\n",
    "\n",
    "    # Ausgabe nach Skalierung (optional)\n",
    "    print(\"\\nBeispiel nach Skalierung (erste Zeilen):\")\n",
    "    print(pd.DataFrame(X_scaled, columns=X.columns).head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Die Datei wurde nicht gefunden. Bitte überprüfen Sie den Pfad.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ein Fehler ist aufgetreten: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ff075-4eef-44d3-b75c-9c752e70ee71",
   "metadata": {},
   "source": [
    "Modelle vergleichen mit Standardeinstellungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bf2b1-446a-4291-9136-bafa0fc4a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def repeated_k_fold(model, X, y, n_splits=5, n_repeats=10):\n",
    "    \"\"\"Führt repeated k-fold cross-validation durch und berechnet die Metriken.\"\"\"\n",
    "    # Repeated Stratified K-Fold\n",
    "    rkf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
    "\n",
    "    # Metriken definieren\n",
    "    accuracy_train = []\n",
    "    accuracy_test = []\n",
    "    f1 = []\n",
    "    roc_auc = []\n",
    "\n",
    "    for train_index, test_index in rkf.split(X, y):\n",
    "        # Splitte die Daten in Trainings- und Testdaten\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Trainiere das Modell\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Vorhersagen für Trainings- und Testdaten\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Berechne die Metriken\n",
    "        accuracy_train.append(accuracy_score(y_train, y_train_pred))\n",
    "        accuracy_test.append(accuracy_score(y_test, y_test_pred))\n",
    "        f1.append(f1_score(y_test, y_test_pred))\n",
    "        roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "    # Durchschnittswerte berechnen\n",
    "    return {\n",
    "        \"Train Accuracy\": np.mean(accuracy_train),\n",
    "        \"Test Accuracy\": np.mean(accuracy_test),\n",
    "        \"F1-Score\": np.mean(f1),\n",
    "        \"ROC-AUC\": np.mean(roc_auc),\n",
    "    }\n",
    "\n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\ml_algorithmus_finden.xlsx\"\n",
    "\n",
    "# Excel-Datei einlesen\n",
    "try:\n",
    "    # Direkt die Datei einlesen (erstes Tabellenblatt standardmäßig)\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Zielvariable (y) und Features (X) extrahieren\n",
    "    y = df['Verletzungsstatus']\n",
    "    X = df.drop(columns=['Verletzungsstatus'])  # Entferne die Zielvariable aus den Features\n",
    "\n",
    "    # Skalierung der Features (X) mit StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)  # Normiert X auf den Standardbereich (Mittelwert = 0, Std = 1)\n",
    "\n",
    "    # Modelle definieren\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42),\n",
    "        \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "        \"SVC\": SVC(probability=True, random_state=42),\n",
    "        \"k-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "        \"MLP Classifier\": MLPClassifier(max_iter=1000, random_state=42),\n",
    "        \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "        \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(),\n",
    "        \"Quadratic Discriminant Analysis\": QuadraticDiscriminantAnalysis(),\n",
    "        \"Bagging Classifier\": BaggingClassifier(random_state=42),\n",
    "        \"Extra Trees\": ExtraTreesClassifier(random_state=42),\n",
    "    }\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    results = []\n",
    "\n",
    "    # Validierung jedes Modells\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Modell wird validiert: {model_name}\")\n",
    "        metrics = repeated_k_fold(model, X_scaled, y)  # Verwende X_scaled statt X\n",
    "        results.append({\"Model\": model_name, **metrics})\n",
    "\n",
    "    # Ergebnisse in DataFrame konvertieren und sortieren\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(by=\"ROC-AUC\", ascending=False)\n",
    "\n",
    "    # Ergebnisse anzeigen\n",
    "    print(\"\\nErgebnisse der Modelle:\")\n",
    "    print(results_df)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Die Datei wurde nicht gefunden. Bitte überprüfen Sie den Pfad.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ein Fehler ist aufgetreten: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2f149-1092-4d73-8353-58d4acda9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e17296a-cb80-46f1-8203-137be8cd83bf",
   "metadata": {},
   "source": [
    "Auto ML mit TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80365055-92f5-4e7e-9d44-8122e2ef858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Daten einlesen\n",
    "file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\ml_algorithmus_finden.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Zielvariable (y) und Features (X) extrahieren\n",
    "y = df['Verletzungsstatus']\n",
    "X = df.drop(columns=['Verletzungsstatus'])\n",
    "\n",
    "# Skalierung der Features (X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Trainings- und Testdaten splitten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TPOTClassifier erstellen und anpassen\n",
    "tpot = TPOTClassifier(verbosity=2, generations=5, population_size=20, random_state=42)\n",
    "\n",
    "# Modell mit dem Trainingsdatensatz optimieren\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Beste Lösung ausgeben\n",
    "print(\"Beste Lösung:\", tpot.fitted_pipeline_)\n",
    "\n",
    "# Evaluation des besten Modells auf den Testdaten\n",
    "print(f\"Test Accuracy: {tpot.score(X_test, y_test)}\")\n",
    "\n",
    "# Optional: Export des besten Modells als Python-Code\n",
    "tpot.export('best_model.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a814cad-fd23-4a57-a987-0493793e958d",
   "metadata": {},
   "source": [
    "Einzelnes Modell validieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e8a42-31ea-48ca-8b7d-c625d7997317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def repeated_k_fold(model, X, y, n_splits=5, n_repeats=10):\n",
    "    \"\"\"Führt repeated k-fold cross-validation durch und berechnet die Metriken.\"\"\"\n",
    "    # Repeated Stratified K-Fold\n",
    "    rkf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
    "\n",
    "    # Metriken definieren\n",
    "    accuracy_train = []\n",
    "    accuracy_test = []\n",
    "    f1 = []\n",
    "    roc_auc = []\n",
    "\n",
    "    for train_index, test_index in rkf.split(X, y):\n",
    "        # Splitte die Daten in Trainings- und Testdaten\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Trainiere das Modell\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Vorhersagen für Trainings- und Testdaten\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Berechne die Metriken\n",
    "        accuracy_train.append(accuracy_score(y_train, y_train_pred))\n",
    "        accuracy_test.append(accuracy_score(y_test, y_test_pred))\n",
    "        f1.append(f1_score(y_test, y_test_pred))\n",
    "        roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "    # Durchschnittswerte berechnen\n",
    "    return {\n",
    "        \"Train Accuracy\": np.mean(accuracy_train),\n",
    "        \"Test Accuracy\": np.mean(accuracy_test),\n",
    "        \"F1-Score\": np.mean(f1),\n",
    "        \"ROC-AUC\": np.mean(roc_auc),\n",
    "    }\n",
    "\n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\ml_algorithmus_finden.xlsx\"\n",
    "\n",
    "# Excel-Datei einlesen\n",
    "try:\n",
    "    # Direkt die Datei einlesen (erstes Tabellenblatt standardmäßig)\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Zielvariable (y) und Features (X) extrahieren\n",
    "    y = df['Verletzungsstatus']\n",
    "    X = df.drop(columns=['Verletzungsstatus'])  # Entferne die Zielvariable aus den Features\n",
    "\n",
    "    # Skalierung der Features (X) mit StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)  # Normiert X auf den Standardbereich (Mittelwert = 0, Std = 1)\n",
    "\n",
    "    # Modelle definieren\n",
    "    models = {\n",
    "    \"Extra Trees\": ExtraTreesClassifier(\n",
    "        bootstrap=False, \n",
    "        criterion=\"gini\", \n",
    "        max_features=0.3, \n",
    "        min_samples_leaf=4, \n",
    "        min_samples_split=9, \n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    ),}\n",
    "\n",
    "\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    results = []\n",
    "\n",
    "    # Validierung jedes Modells\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Modell wird validiert: {model_name}\")\n",
    "        metrics = repeated_k_fold(model, X_scaled, y)  # Verwende X_scaled statt X\n",
    "        results.append({\"Model\": model_name, **metrics})\n",
    "\n",
    "    # Ergebnisse in DataFrame konvertieren und sortieren\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(by=\"ROC-AUC\", ascending=False)\n",
    "\n",
    "    # Ergebnisse anzeigen\n",
    "    print(\"\\nErgebnisse der Modelle:\")\n",
    "    print(results_df)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Die Datei wurde nicht gefunden. Bitte überprüfen Sie den Pfad.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ein Fehler ist aufgetreten: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e6739-e7f5-43fa-840f-f2f498f7b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b954e-4f7c-427d-a97e-0bd670938755",
   "metadata": {},
   "source": [
    "Grid Search für ein ausgewähltes Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b13b7-72c5-412b-9066-83e5627aec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "    \n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\ml_algorithmus_finden.xlsx\"\n",
    "\n",
    "# Excel-Datei einlesen\n",
    "try:\n",
    "    # Direkt die Datei einlesen (erstes Tabellenblatt standardmäßig)\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Zielvariable (y) und Features (X) extrahieren\n",
    "    y = df['Verletzungsstatus']\n",
    "    X = df.drop(columns=['Verletzungsstatus'])  # Entferne die Zielvariable aus den Features\n",
    "\n",
    "    # Skalierung der Features (X) mit StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)  # Normiert X auf den Standardbereich (Mittelwert = 0, Std = 1)\n",
    "\n",
    "    # Ausgabe nach Skalierung (optional)\n",
    "    print(\"\\nBeispiel nach Skalierung (erste Zeilen):\")\n",
    "    print(pd.DataFrame(X_scaled, columns=X.columns).head())\n",
    "    \n",
    "    # Train-Test-Split durchführen (80% Training, 20% Test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Modell initialisieren\n",
    "    model = ExtraTreesClassifier(random_state=42)\n",
    "    \n",
    "    # Parameterbereich für GridSearch definieren\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200, 300, 500],\n",
    "        'max_features': ['auto', 'sqrt', 'log2', 0.1, 0.3],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 10, 20, 30, 40],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 5, 10],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "    \n",
    "    # GridSearchCV initialisieren\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                               cv=5, n_jobs=-1, scoring='roc_auc', verbose=1)\n",
    "    \n",
    "    # Führe GridSearchCV aus auf dem Trainingsdatensatz\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Beste Parameter und beste Leistung anzeigen\n",
    "    print(f\"Beste Parameter: {grid_search.best_params_}\")\n",
    "    print(f\"Beste ROC-AUC: {grid_search.best_score_}\")\n",
    "    \n",
    "    # Optional: Testdaten mit dem besten Modell evaluieren\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Vorhersage und Bewertung auf dem Testdatensatz\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    y_test_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_test_prob)\n",
    "    \n",
    "    print(f\"ROC-AUC auf den Testdaten: {roc_auc}\")\n",
    "    \n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Die Datei wurde nicht gefunden. Bitte überprüfen Sie den Pfad.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ein Fehler ist aufgetreten: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7b41d-b7c0-4dda-85fc-d90640d1e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())  # Gibt den aktuellen Arbeitsverzeichnis-Pfad aus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf108d-6749-4ba9-bf01-b21cc293d6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
