{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ba5fc4-fc84-49ce-92d8-7f55a031df13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell wird validiert: Logistic Regression\n",
      "Modell wird validiert: Decision Tree\n",
      "Modell wird validiert: Random Forest\n",
      "Modell wird validiert: Gradient Boosting\n",
      "Modell wird validiert: XGBoost\n",
      "Modell wird validiert: LightGBM\n",
      "Modell wird validiert: SVC\n",
      "Modell wird validiert: k-Nearest Neighbors\n",
      "Modell wird validiert: MLP Classifier\n",
      "Modell wird validiert: Gaussian Naive Bayes\n",
      "Modell wird validiert: Linear Discriminant Analysis\n",
      "Modell wird validiert: Quadratic Discriminant Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\boehmer\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell wird validiert: Bagging Classifier\n",
      "Modell wird validiert: Extra Trees\n",
      "\n",
      "Ergebnisse der Modelle:\n",
      "                              Model   Train Accuracy    Test Accuracy       F1-Score_1         Recall_1        ROC-AUC_1\n",
      "0               Logistic Regression  1.0000 ± 0.0000  0.8895 ± 0.0572  0.8953 ± 0.0589  0.8914 ± 0.1000  0.9541 ± 0.0461\n",
      "8                    MLP Classifier  1.0000 ± 0.0000  0.8604 ± 0.0590  0.8651 ± 0.0641  0.8458 ± 0.1053  0.9441 ± 0.0471\n",
      "4                           XGBoost  1.0000 ± 0.0000  0.8416 ± 0.0656  0.8454 ± 0.0701  0.8162 ± 0.1068  0.9328 ± 0.0376\n",
      "6                               SVC  0.9777 ± 0.0090  0.8381 ± 0.0692  0.8502 ± 0.0653  0.8559 ± 0.0961  0.9325 ± 0.0454\n",
      "13                      Extra Trees  1.0000 ± 0.0000  0.8153 ± 0.0775  0.8263 ± 0.0768  0.8214 ± 0.1040  0.9268 ± 0.0523\n",
      "2                     Random Forest  1.0000 ± 0.0000  0.8180 ± 0.0746  0.8291 ± 0.0713  0.8232 ± 0.0997  0.9209 ± 0.0486\n",
      "3                 Gradient Boosting  1.0000 ± 0.0000  0.8310 ± 0.0825  0.8400 ± 0.0771  0.8224 ± 0.0974  0.9189 ± 0.0573\n",
      "5                          LightGBM  1.0000 ± 0.0000  0.8230 ± 0.0648  0.8281 ± 0.0719  0.8047 ± 0.1103  0.9169 ± 0.0453\n",
      "9              Gaussian Naive Bayes  0.9049 ± 0.0211  0.8395 ± 0.0619  0.8565 ± 0.0526  0.8806 ± 0.0695  0.9136 ± 0.0628\n",
      "12               Bagging Classifier  0.9843 ± 0.0141  0.8045 ± 0.0742  0.8004 ± 0.0861  0.7454 ± 0.1235  0.8908 ± 0.0630\n",
      "7               k-Nearest Neighbors  0.8398 ± 0.0296  0.7771 ± 0.0786  0.7446 ± 0.1144  0.6297 ± 0.1398  0.8727 ± 0.0588\n",
      "1                     Decision Tree  1.0000 ± 0.0000  0.7940 ± 0.0783  0.8032 ± 0.0793  0.7933 ± 0.1230  0.7938 ± 0.0799\n",
      "10     Linear Discriminant Analysis  1.0000 ± 0.0000  0.6957 ± 0.0968  0.7014 ± 0.1038  0.6778 ± 0.1430  0.7491 ± 0.1049\n",
      "11  Quadratic Discriminant Analysis  1.0000 ± 0.0000  0.5378 ± 0.0903  0.5662 ± 0.1340  0.6059 ± 0.2290  0.5312 ± 0.0917\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "def repeated_k_fold(model, X, y, n_splits=5, n_repeats=10):\n",
    "    rkf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
    "\n",
    "    accuracy_train, accuracy_test = [], []\n",
    "    f1, recall, roc_auc = [], [], []\n",
    "\n",
    "    for train_index, test_index in rkf.split(X, y):\n",
    "        # Aufteilen in Trainings- und Testdaten\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Skalierung: nur an den Trainingsdaten fitten\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train_scaled)\n",
    "        y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        accuracy_train.append(accuracy_score(y_train, y_train_pred))\n",
    "        accuracy_test.append(accuracy_score(y_test, y_test_pred))\n",
    "        f1.append(f1_score(y_test, y_test_pred))\n",
    "        recall.append(recall_score(y_test, y_test_pred))\n",
    "        roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test_scaled)[:, 1]))\n",
    "\n",
    "    return {\n",
    "        \"Train Accuracy\": (np.mean(accuracy_train), np.std(accuracy_train)),\n",
    "        \"Test Accuracy\": (np.mean(accuracy_test), np.std(accuracy_test)),\n",
    "        \"F1-Score_1\": (np.mean(f1), np.std(f1)),\n",
    "        \"Recall_1\": (np.mean(recall), np.std(recall)),\n",
    "        \"ROC-AUC_1\": (np.mean(roc_auc), np.std(roc_auc)),\n",
    "    }\n",
    "\n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\Basistabelle\\Basistabelle_ML_neu.xlsx\"\n",
    "\n",
    "try: \n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Zielvariable (y) und Features (X) extrahieren\n",
    "    y = df['Verletzungsstatus']\n",
    "    \n",
    "    # Dummy-Variable \"Geschlecht_weiblich\" separieren, falls vorhanden\n",
    "    if 'Geschlecht_weiblich' in df.columns:\n",
    "        geschlecht_weiblich = df[['Geschlecht_weiblich']]\n",
    "        X = df.drop(columns=['Verletzungsstatus', 'Geschlecht_weiblich'])\n",
    "        # Die Dummy-Variable wieder anhängen\n",
    "        X = np.hstack((X.values, geschlecht_weiblich.values))\n",
    "    else:\n",
    "        X = df.drop(columns=['Verletzungsstatus']).values\n",
    "    \n",
    "    # Modelle definieren\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(eval_metric=\"logloss\", random_state=42),\n",
    "        \"LightGBM\": LGBMClassifier(verbose=-1, random_state=42),\n",
    "        \"SVC\": SVC(probability=True, random_state=42),\n",
    "        \"k-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "        \"MLP Classifier\": MLPClassifier(max_iter=1000, random_state=42),\n",
    "        \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "        \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(),\n",
    "        \"Quadratic Discriminant Analysis\": QuadraticDiscriminantAnalysis(),\n",
    "        \"Bagging Classifier\": BaggingClassifier(random_state=42),\n",
    "        \"Extra Trees\": ExtraTreesClassifier(random_state=42),\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Modell wird validiert: {model_name}\")\n",
    "        metrics = repeated_k_fold(model, X, y)\n",
    "        \n",
    "        formatted_metrics = {\n",
    "            \"Model\": model_name,\n",
    "            \"Train Accuracy\": f\"{metrics['Train Accuracy'][0]:.4f} ± {metrics['Train Accuracy'][1]:.4f}\",\n",
    "            \"Test Accuracy\": f\"{metrics['Test Accuracy'][0]:.4f} ± {metrics['Test Accuracy'][1]:.4f}\",\n",
    "            \"F1-Score_1\": f\"{metrics['F1-Score_1'][0]:.4f} ± {metrics['F1-Score_1'][1]:.4f}\",\n",
    "            \"Recall_1\": f\"{metrics['Recall_1'][0]:.4f} ± {metrics['Recall_1'][1]:.4f}\",\n",
    "            \"ROC-AUC_1\": f\"{metrics['ROC-AUC_1'][0]:.4f} ± {metrics['ROC-AUC_1'][1]:.4f}\",\n",
    "        }\n",
    "        results.append(formatted_metrics)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(by=\"ROC-AUC_1\", ascending=False)\n",
    "    \n",
    "    print(\"\\nErgebnisse der Modelle:\")\n",
    "    print(results_df)\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Die Datei wurde nicht gefunden. Bitte überprüfe den Pfad.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ein Fehler ist aufgetreten: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef801448-93ef-48df-bdcf-5b6dc051af87",
   "metadata": {},
   "source": [
    "Datenaugmentation (Datenleakage gänzlich vermeiden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb9e8c55-d72f-4cf8-a912-956a9ee24ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "\n",
    "def perform_clustering(X_sub, max_clusters=4):\n",
    "    \"\"\"\n",
    "    Führt eine Clusteranalyse auf den numerischen Parametern (außer den\n",
    "    Gruppierungsvariablen \"Verletzungsstatus\" und \"Geschlecht_weiblich\")\n",
    "    durch und teilt diese in max_clusters Gruppen ein.\n",
    "    \"\"\"\n",
    "    # Wähle alle numerischen Spalten außer \"Verletzungsstatus\" und \"Geschlecht_weiblich\"\n",
    "    cols = [col for col in X_sub.columns if col not in [\"Verletzungsstatus\", \"Geschlecht_weiblich\"]]\n",
    "    X_num = X_sub[cols]\n",
    "    \n",
    "    # Berechne die Korrelationsmatrix und transformiere sie in eine Distanzmatrix\n",
    "    corr = X_num.corr().abs()\n",
    "    dist = np.clip(1 - corr, a_min=0, a_max=None)  # Clippe negative Werte auf 0\n",
    "    \n",
    "    # Konvertiere die Distanzmatrix in ein 1D-Array (upper triangular)\n",
    "    dists = squareform(dist.values)\n",
    "    \n",
    "    # Hierarchische Clusteranalyse (z. B. mit der Ward-Methode)\n",
    "    Z = linkage(dists, method='ward')\n",
    "    \n",
    "    # Führe fcluster aus, um die Spalten in max_clusters Cluster zu unterteilen\n",
    "    cluster_labels = fcluster(Z, max_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Ordne den Spalten Clusterlabels zu\n",
    "    clusters = {}\n",
    "    for col, label in zip(cols, cluster_labels):\n",
    "        clusters.setdefault(label, []).append(col)\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def augment_subgroup(X_sub, clusters, p_augment=0.7):\n",
    "    \"\"\"\n",
    "    Augmentiert die Daten in X_sub (einen DataFrame für eine Subgruppe)\n",
    "    für alle in clusters definierten Parametergruppen.\n",
    "    \n",
    "    Für jede Zeile und für jede Cluster-Gruppe wird mit Wahrscheinlichkeit p_augment\n",
    "    ein neuer Wert gezogen – ansonsten wird der Originalwert beibehalten.\n",
    "    Dabei werden für jede Parametergruppe der Mittelwertvektor und die Kovarianzmatrix\n",
    "    des gesamten Subdatensatzes berechnet.\n",
    "    \n",
    "    Rückgabe: DataFrame mit augmentierten Zeilen (1 augmentiertes Sample pro Originalzeile)\n",
    "    \"\"\"\n",
    "    augmented_rows = []\n",
    "    \n",
    "    # Für jede Cluster-Gruppe: Berechne Mittelwert und Kovarianzmatrix\n",
    "    cluster_params = {}\n",
    "    for cl_id, cols in clusters.items():\n",
    "        cluster_data = X_sub[cols]\n",
    "        mu = cluster_data.mean().values\n",
    "        # Falls nur ein Feature im Cluster ist, gibt np.cov einen Skalar zurück.\n",
    "        if len(cols) == 1:\n",
    "            cov = np.cov(cluster_data.values.flatten(), ddof=0)\n",
    "            cov = np.atleast_2d(cov)\n",
    "        else:\n",
    "            cov = np.cov(cluster_data.values, rowvar=False)\n",
    "        cluster_params[cl_id] = (mu, cov, cols)\n",
    "    \n",
    "    # Gehe jede Zeile (Originaldatensatz) durch\n",
    "    for idx, row in X_sub.iterrows():\n",
    "        new_row = row.copy()\n",
    "        # Für jede Cluster-Gruppe stochastisch augmentieren\n",
    "        for cl_id, (mu, cov, cols) in cluster_params.items():\n",
    "            if np.random.rand() < p_augment:\n",
    "                # Ziehe einen neuen Vektor aus der multivariaten Normalverteilung\n",
    "                new_values = np.random.multivariate_normal(mu, cov)\n",
    "                # Überschreibe nur die Werte in den entsprechenden Spalten\n",
    "                for col, val in zip(cols, new_values):\n",
    "                    new_row[col] = val\n",
    "        augmented_rows.append(new_row)\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    \n",
    "    # Sicherstellen, dass Gruppierungsvariablen den richtigen Datentyp behalten:\n",
    "    for col in [\"Geschlecht_weiblich\", \"Verletzungsstatus\"]:\n",
    "        if col in augmented_df.columns:\n",
    "            augmented_df[col] = augmented_df[col].astype(int)\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Funktion: Stratifikation und Augmentation der Trainingsdaten\n",
    "# ----------------------------------------------------------------\n",
    "def augment_training_data(X_train, y_train, max_clusters=4, p_augment=0.7, num_new_samples=1):\n",
    "    \"\"\"\n",
    "    Führt zuerst eine Stratifikation des Trainingsdatensatzes nach \"Verletzungsstatus\"\n",
    "    und \"Geschlecht_weiblich\" durch und wendet dann in jeder Subgruppe:\n",
    "      - eine Clusteranalyse (auf alle übrigen numerischen Features)\n",
    "      - die Augmentation (Ziehung neuer Samples aus multivariater Normalverteilung)\n",
    "    an.\n",
    "    \n",
    "    num_new_samples gibt an, wie viele augmentierte Samples pro Originalzeile generiert werden.\n",
    "    \"\"\"\n",
    "    # Kombiniere X_train und y_train, damit wir gruppieren können\n",
    "    df_train = X_train.copy()\n",
    "    df_train[\"Verletzungsstatus\"] = y_train\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Stratifikation: Erstelle eine Gruppierung nach (Verletzungsstatus, Geschlecht_weiblich)\n",
    "    augmented_groups = []\n",
    "    for key, group_df in df_train.groupby([\"Verletzungsstatus\", \"Geschlecht_weiblich\"]):\n",
    "        group_df = group_df.reset_index(drop=True)\n",
    "        # Führe Clusteranalyse auf dieser Subgruppe durch\n",
    "        clusters = perform_clustering(group_df, max_clusters=max_clusters)\n",
    "        # Erzeuge für diese Subgruppe num_new_samples augmentierte Samples pro Zeile\n",
    "        aug_list = []\n",
    "        for _ in range(num_new_samples):\n",
    "            aug = augment_subgroup(group_df, clusters, p_augment=p_augment)\n",
    "            aug_list.append(aug)\n",
    "        aug_group = pd.concat(aug_list, axis=0)\n",
    "        augmented_groups.append(aug_group)\n",
    "    \n",
    "    # Vereinige alle augmentierten Subgruppen\n",
    "    augmented_df = pd.concat(augmented_groups, axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # Trenne Zielvariable und Features (hier war \"Verletzungsstatus\" in df_train enthalten)\n",
    "    y_aug = augmented_df[\"Verletzungsstatus\"]\n",
    "    X_aug = augmented_df.drop(columns=[\"Verletzungsstatus\"])\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Integration in den Trainingssplit (innerer CV) mit Augmentation\n",
    "# ----------------------------------------------------------------\n",
    "def inner_cv_with_augmentation(X_train, y_train):\n",
    "    from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    inner_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, test_idx in inner_cv.split(X_train, y_train):\n",
    "        # Kopien der Daten erstellen\n",
    "        X_inner_train = X_train.iloc[train_idx].copy()\n",
    "        X_inner_test = X_train.iloc[test_idx].copy()\n",
    "        y_inner_train, y_inner_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "        # Normalisierung innerhalb des Folds:\n",
    "        dummy = \"Geschlecht_weiblich\"\n",
    "        cols_to_scale = [col for col in X_inner_train.columns if col != dummy]\n",
    "        scaler = StandardScaler()\n",
    "        X_inner_train[cols_to_scale] = scaler.fit_transform(X_inner_train[cols_to_scale])\n",
    "        X_inner_test[cols_to_scale] = scaler.transform(X_inner_test[cols_to_scale])\n",
    "\n",
    "        # Augmentation auf den normalisierten Trainingsdaten durchführen:\n",
    "        X_inner_train_aug, y_inner_train_aug = augment_training_data(\n",
    "            X_inner_train, y_inner_train, max_clusters=4, p_augment=0.7, num_new_samples=3\n",
    "        )\n",
    "\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        # Auch hier wird cross_val_score innerhalb der Trainingsdaten-CV eingesetzt\n",
    "        score = cross_val_score(model, X_inner_train_aug, y_inner_train_aug, cv=3,\n",
    "                                scoring=\"roc_auc\", n_jobs=-1).mean()\n",
    "        scores.append(score)\n",
    "\n",
    "    avg_score = np.mean(scores)\n",
    "    #print(\"Durchschnittliche innere CV-ROC-AUC mit Augmentation:\", avg_score)\n",
    "    return avg_score\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# Integration in den Trainingssplit (innerer CV) ohne Augmentation\n",
    "#------------------------------------------------------------------------\n",
    "def inner_cv_without_augmentation(X_train, y_train):\n",
    "    from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    inner_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, test_idx in inner_cv.split(X_train, y_train):\n",
    "        # Kopien der Daten erstellen, um spätere Änderungen lokal zu halten\n",
    "        X_inner_train = X_train.iloc[train_idx].copy()\n",
    "        X_inner_test = X_train.iloc[test_idx].copy()\n",
    "        y_inner_train, y_inner_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "        # Normalisierung innerhalb des Folds:\n",
    "        dummy = \"Geschlecht_weiblich\"\n",
    "        cols_to_scale = [col for col in X_inner_train.columns if col != dummy]\n",
    "        scaler = StandardScaler()\n",
    "        X_inner_train[cols_to_scale] = scaler.fit_transform(X_inner_train[cols_to_scale])\n",
    "        X_inner_test[cols_to_scale] = scaler.transform(X_inner_test[cols_to_scale])\n",
    "\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        # cross_val_score führt hier eine weitere CV auf den inneren Trainingsdaten durch\n",
    "        score = cross_val_score(model, X_inner_train, y_inner_train, cv=3,\n",
    "                                scoring=\"roc_auc\", n_jobs=-1).mean()\n",
    "        scores.append(score)\n",
    "    \n",
    "    avg_score = np.mean(scores)\n",
    "    #print(\"Durchschnittliche innere CV-ROC-AUC ohne Augmentation:\", avg_score)\n",
    "    return avg_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a22a196-324e-42fb-ab87-5e5d0652f915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vergleich: Innere CV ohne vs. mit Augmentation ===\n",
      "Durchschnittliche innere CV-ROC-AUC ohne Augmentation: 0.9261234109028226\n",
      "Durchschnittliche innere CV-ROC-AUC mit Augmentation: 0.9763723972941868\n",
      "\n",
      "Zusammenfassung der Ergebnisse:\n",
      "ROC-AUC ohne Augmentation: 0.9261234109028226\n",
      "ROC-AUC mit Augmentation: 0.9763723972941868\n",
      "Differenz: 0.050248986391364214\n"
     ]
    }
   ],
   "source": [
    "# Originaldaten laden\n",
    "file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\Basistabelle\\Basistabelle_ML_neu.xlsx\"\n",
    "df_original = pd.read_excel(file_path)\n",
    "\n",
    "# Aufteilen in Features und Zielvariable\n",
    "X_original = df_original.drop(columns=[\"Verletzungsstatus\"])\n",
    "y_original = df_original[\"Verletzungsstatus\"]\n",
    "\n",
    "# Vergleich beider Ansätze:\n",
    "print(\"=== Vergleich: Innere CV ohne vs. mit Augmentation ===\")\n",
    "score_without = inner_cv_without_augmentation(X_original, y_original)\n",
    "score_with = inner_cv_with_augmentation(X_original, y_original)\n",
    "\n",
    "print(\"\\nZusammenfassung der Ergebnisse:\")\n",
    "print(\"Durchschnittliche innere CV-ROC-AUC ohne Augmentation:\", score_without)\n",
    "print(\"Durchschnittliche innere CV-ROC-AUC mit Augmentation:\", score_with)\n",
    "print(\"Differenz:\", score_with - score_without)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d469e3c0-29cb-40a9-a0a6-48bd115ab846",
   "metadata": {},
   "source": [
    "Test-Datenaugmentation-Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66366fd3-46b7-4c20-b3cd-fd458261f8e2",
   "metadata": {},
   "source": [
    "Test Cluster-Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004bac5-db09-42c8-8a1a-723872b6ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(X_sub, max_clusters=4):\n",
    "    \"\"\"\n",
    "    Führt eine Clusteranalyse auf den numerischen Parametern (außer den\n",
    "    Gruppierungsvariablen \"Verletzungsstatus\" und \"Geschlecht_weiblich\")\n",
    "    durch und teilt diese in max_clusters Gruppen ein.\n",
    "    \"\"\"\n",
    "    # Wähle alle numerischen Spalten außer \"Verletzungsstatus\" und \"Geschlecht_weiblich\"\n",
    "    cols = [col for col in X_sub.columns if col not in [\"Verletzungsstatus\", \"Geschlecht_weiblich\"]]\n",
    "    X_num = X_sub[cols]\n",
    "    \n",
    "    # Berechne die Korrelationsmatrix und transformiere sie in eine Distanzmatrix\n",
    "    corr = X_num.corr().abs()\n",
    "    dist = np.clip(1 - corr, a_min=0, a_max=None)  # Clippe negative Werte auf 0\n",
    "    \n",
    "    # Konvertiere die Distanzmatrix in ein 1D-Array (upper triangular)\n",
    "    dists = squareform(dist.values)\n",
    "    \n",
    "    # Hierarchische Clusteranalyse (z. B. mit der Ward-Methode)\n",
    "    Z = linkage(dists, method='ward')\n",
    "    \n",
    "    # Führe fcluster aus, um die Spalten in max_clusters Cluster zu unterteilen\n",
    "    cluster_labels = fcluster(Z, max_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Ordne den Spalten Clusterlabels zu\n",
    "    clusters = {}\n",
    "    for col, label in zip(cols, cluster_labels):\n",
    "        clusters.setdefault(label, []).append(col)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee68393-1b04-44d8-b09a-5a4bda7be820",
   "metadata": {},
   "source": [
    "Test Subgruppen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf084d1-1477-4966-84f3-8b8b5f494077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_subgroup(X_sub, clusters, p_augment=0.7):\n",
    "    \"\"\"\n",
    "    Augmentiert die Daten in X_sub (einen DataFrame für eine Subgruppe)\n",
    "    für alle in clusters definierten Parametergruppen.\n",
    "    \n",
    "    Für jede Zeile und für jede Cluster-Gruppe wird mit Wahrscheinlichkeit p_augment\n",
    "    ein neuer Wert gezogen – ansonsten wird der Originalwert beibehalten.\n",
    "    Dabei werden für jede Parametergruppe der Mittelwertvektor und die Kovarianzmatrix\n",
    "    des gesamten Subdatensatzes berechnet.\n",
    "    \n",
    "    Rückgabe: DataFrame mit augmentierten Zeilen (1 augmentiertes Sample pro Originalzeile)\n",
    "    \"\"\"\n",
    "    augmented_rows = []\n",
    "    \n",
    "    # Für jede Cluster-Gruppe: Berechne Mittelwert und Kovarianzmatrix\n",
    "    cluster_params = {}\n",
    "    for cl_id, cols in clusters.items():\n",
    "        cluster_data = X_sub[cols]\n",
    "        mu = cluster_data.mean().values\n",
    "        # Falls nur ein Feature im Cluster ist, gibt np.cov einen Skalar zurück.\n",
    "        if len(cols) == 1:\n",
    "            cov = np.cov(cluster_data.values.flatten(), ddof=0)\n",
    "            cov = np.atleast_2d(cov)\n",
    "        else:\n",
    "            cov = np.cov(cluster_data.values, rowvar=False)\n",
    "        cluster_params[cl_id] = (mu, cov, cols)\n",
    "    \n",
    "    # Gehe jede Zeile (Originaldatensatz) durch\n",
    "    for idx, row in X_sub.iterrows():\n",
    "        new_row = row.copy()\n",
    "        # Für jede Cluster-Gruppe stochastisch augmentieren\n",
    "        for cl_id, (mu, cov, cols) in cluster_params.items():\n",
    "            if np.random.rand() < p_augment:\n",
    "                # Ziehe einen neuen Vektor aus der multivariaten Normalverteilung\n",
    "                new_values = np.random.multivariate_normal(mu, cov)\n",
    "                # Überschreibe nur die Werte in den entsprechenden Spalten\n",
    "                for col, val in zip(cols, new_values):\n",
    "                    new_row[col] = val\n",
    "        augmented_rows.append(new_row)\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    \n",
    "    # Sicherstellen, dass Gruppierungsvariablen den richtigen Datentyp behalten:\n",
    "    for col in [\"Geschlecht_weiblich\", \"Verletzungsstatus\"]:\n",
    "        if col in augmented_df.columns:\n",
    "            augmented_df[col] = augmented_df[col].astype(int)\n",
    "    \n",
    "    return augmented_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70159491-a452-4f98-90ef-cf744f5c5cce",
   "metadata": {},
   "source": [
    "Test Daten-Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d331740-1a40-425f-b139-80c4f8722dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_training_data(X_train, y_train, max_clusters=4, p_augment=0.7, num_new_samples=1):\n",
    "    \"\"\"\n",
    "    Führt zuerst eine Stratifikation des Trainingsdatensatzes nach \"Verletzungsstatus\"\n",
    "    und \"Geschlecht_weiblich\" durch und wendet dann in jeder Subgruppe:\n",
    "      - eine Clusteranalyse (auf alle übrigen numerischen Features)\n",
    "      - die Augmentation (Ziehung neuer Samples aus multivariater Normalverteilung)\n",
    "    an.\n",
    "    \n",
    "    num_new_samples gibt an, wie viele augmentierte Samples pro Originalzeile generiert werden.\n",
    "    \"\"\"\n",
    "    # Kombiniere X_train und y_train, damit wir gruppieren können\n",
    "    df_train = X_train.copy()\n",
    "    df_train[\"Verletzungsstatus\"] = y_train\n",
    "    df_train.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Stratifikation: Erstelle eine Gruppierung nach (Verletzungsstatus, Geschlecht_weiblich)\n",
    "    augmented_groups = []\n",
    "    for key, group_df in df_train.groupby([\"Verletzungsstatus\", \"Geschlecht_weiblich\"]):\n",
    "        group_df = group_df.reset_index(drop=True)\n",
    "        # Führe Clusteranalyse auf dieser Subgruppe durch\n",
    "        clusters = perform_clustering(group_df, max_clusters=max_clusters)\n",
    "        # Erzeuge für diese Subgruppe num_new_samples augmentierte Samples pro Zeile\n",
    "        aug_list = []\n",
    "        for _ in range(num_new_samples):\n",
    "            aug = augment_subgroup(group_df, clusters, p_augment=p_augment)\n",
    "            aug_list.append(aug)\n",
    "        aug_group = pd.concat(aug_list, axis=0)\n",
    "        augmented_groups.append(aug_group)\n",
    "    \n",
    "    # Vereinige alle augmentierten Subgruppen\n",
    "    augmented_df = pd.concat(augmented_groups, axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # Trenne Zielvariable und Features (hier war \"Verletzungsstatus\" in df_train enthalten)\n",
    "    y_aug = augmented_df[\"Verletzungsstatus\"]\n",
    "    X_aug = augmented_df.drop(columns=[\"Verletzungsstatus\"])\n",
    "    \n",
    "    return X_aug, y_aug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b9be88-c7c6-4d7a-8610-3dc564cda739",
   "metadata": {},
   "source": [
    "Clusterbildung-Validierung mit Rauschen global und gruppenspezifisch, über 100 wiederholungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea93616-c97c-4df7-9f76-cd26e8187d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Hilfsfunktion: Gruppenschlüssel übersetzen\n",
    "# -----------------------------\n",
    "def map_group_label(key):\n",
    "    \"\"\"\n",
    "    Übersetzt den Gruppenschlüssel (Verletzungsstatus, Geschlecht_weiblich) \n",
    "    in eine lesbare Bezeichnung.\n",
    "    \n",
    "    Verletzungsstatus: 0 -> \"unverletzt\", 1 -> \"verletzt\"\n",
    "    Geschlecht_weiblich: 0 -> \"männlich\", 1 -> \"weiblich\"\n",
    "    \"\"\"\n",
    "    status = \"verletzt\" if key[0] == 1 else \"unverletzt\"\n",
    "    gender = \"weiblich\" if key[1] == 1 else \"männlich\"\n",
    "    return f\"Gruppe ({status}-{gender})\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Funktion: Rauschen hinzufügen\n",
    "# -----------------------------\n",
    "def add_noise_to_data(df, noise_factor=0.1):\n",
    "    \"\"\"\n",
    "    Fügt allen numerischen Spalten (außer \"Geschlecht_weiblich\" und \"Verletzungsstatus\")\n",
    "    Rauschen hinzu, wobei das Rauschen ein Anteil (noise_factor) der ursprünglichen\n",
    "    Standardabweichung beträgt.\n",
    "    \"\"\"\n",
    "    df_noisy = df.copy()\n",
    "    numeric_cols = [col for col in df_noisy.select_dtypes(include=['float64', 'int64']).columns \n",
    "                    if col not in [\"Geschlecht_weiblich\", \"Verletzungsstatus\"]]\n",
    "    for col in numeric_cols:\n",
    "        std = df_noisy[col].std()\n",
    "        noise = np.random.normal(0, std * noise_factor, size=df_noisy.shape[0])\n",
    "        df_noisy[col] += noise\n",
    "    return df_noisy\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Funktion: Clusterbildung\n",
    "# -----------------------------\n",
    "def perform_clustering(X_sub, max_clusters=4):\n",
    "    \"\"\"\n",
    "    Führt eine hierarchische Clusteranalyse auf den numerischen Features (ohne \n",
    "    \"Geschlecht_weiblich\" und \"Verletzungsstatus\") durch und teilt in max_clusters ein.\n",
    "    \"\"\"\n",
    "    cols = [col for col in X_sub.columns if col not in [\"Verletzungsstatus\", \"Geschlecht_weiblich\"]]\n",
    "    X_num = X_sub[cols]\n",
    "    corr = X_num.corr().abs()\n",
    "    dist = np.clip(1 - corr, a_min=0, a_max=None)\n",
    "    dists = squareform(dist.values)\n",
    "    Z = linkage(dists, method='ward')\n",
    "    cluster_labels = fcluster(Z, max_clusters, criterion='maxclust')\n",
    "    clusters = {}\n",
    "    for col, label in zip(cols, cluster_labels):\n",
    "        clusters.setdefault(label, []).append(col)\n",
    "    return clusters\n",
    "\n",
    "# -----------------------------\n",
    "# Hilfsfunktionen für Stabilitätsmetriken und Visualisierung\n",
    "# -----------------------------\n",
    "def get_feature_order(df):\n",
    "    return [col for col in df.columns if col not in [\"Verletzungsstatus\", \"Geschlecht_weiblich\"]]\n",
    "\n",
    "def get_cluster_labels(clusters, feature_order):\n",
    "    feature_cluster = {}\n",
    "    for cl, feats in clusters.items():\n",
    "        for feat in feats:\n",
    "            feature_cluster[feat] = cl\n",
    "    return [feature_cluster.get(feat, -1) for feat in feature_order]\n",
    "\n",
    "def visualize_clusters(df_group, clusters, title=\"Cluster Visualization\"):\n",
    "    \"\"\"\n",
    "    Visualisiert die Cluster der Features mittels PCA (auf der Korrelationsmatrix).\n",
    "    Es werden keine Textannotationen angezeigt, damit die Grafik übersichtlicher bleibt.\n",
    "    \"\"\"\n",
    "    feature_order = get_feature_order(df_group)\n",
    "    X_num = df_group[feature_order]\n",
    "    corr = X_num.corr().values\n",
    "    pca = PCA(n_components=2)\n",
    "    coords = pca.fit_transform(corr)\n",
    "    labels = get_cluster_labels(clusters, feature_order)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    scatter = plt.scatter(coords[:,0], coords[:,1], c=labels, cmap='viridis', s=100)\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar(scatter, label=\"Cluster\")\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Funktionen für Gruppierung und Vergleich\n",
    "# -----------------------------\n",
    "def get_group_clusters(df):\n",
    "    \"\"\"\n",
    "    Gruppiert den Datensatz anhand von (\"Verletzungsstatus\", \"Geschlecht_weiblich\")\n",
    "    und berechnet für jede Gruppe die Cluster.\n",
    "    \"\"\"\n",
    "    groups = {}\n",
    "    for key, group in df.groupby([\"Verletzungsstatus\", \"Geschlecht_weiblich\"]):\n",
    "        group = group.reset_index(drop=True)\n",
    "        clusters = perform_clustering(group, max_clusters=4)\n",
    "        groups[key] = clusters\n",
    "    return groups\n",
    "\n",
    "def compute_stability_metrics(clusters_1, clusters_2, feature_order):\n",
    "    \"\"\"\n",
    "    Vergleicht zwei Clusterlösungen anhand von ARI und NMI.\n",
    "    \"\"\"\n",
    "    labels_1 = get_cluster_labels(clusters_1, feature_order)\n",
    "    labels_2 = get_cluster_labels(clusters_2, feature_order)\n",
    "    ari = adjusted_rand_score(labels_1, labels_2)\n",
    "    nmi = normalized_mutual_info_score(labels_1, labels_2)\n",
    "    return ari, nmi\n",
    "\n",
    "def compute_pairwise_metrics(solution_list, feature_order):\n",
    "    \"\"\"\n",
    "    Berechnet pairwise ARI und NMI über eine Liste von Clusterlösungen.\n",
    "    Gibt (mean_ARI, std_ARI, mean_NMI, std_NMI) zurück.\n",
    "    \"\"\"\n",
    "    aris = []\n",
    "    nmis = []\n",
    "    n = len(solution_list)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            ari, nmi = compute_stability_metrics(solution_list[i], solution_list[j], feature_order)\n",
    "            aris.append(ari)\n",
    "            nmis.append(nmi)\n",
    "    return np.mean(aris), np.std(aris), np.mean(nmis), np.std(nmis)\n",
    "\n",
    "def compute_pairwise_consistency(solution_list):\n",
    "    \"\"\"\n",
    "    Berechnet pairwise Cluster-Konsistenz (durchschnittlicher Match-Prozentsatz) über eine Liste von Clusterlösungen.\n",
    "    \"\"\"\n",
    "    consistencies = []\n",
    "    n = len(solution_list)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            cons = average_cluster_consistency(solution_list[i], solution_list[j])\n",
    "            consistencies.append(cons)\n",
    "    return np.mean(consistencies), np.std(consistencies)\n",
    "\n",
    "def average_cluster_consistency(clusters_1, clusters_2):\n",
    "    \"\"\"\n",
    "    Berechnet für einen Vergleich zweier Clusterlösungen den durchschnittlichen Match-Prozentsatz.\n",
    "    Für jeden Cluster in clusters_1 wird der beste Matching-Cluster in clusters_2 gesucht.\n",
    "    \"\"\"\n",
    "    match_percentages = []\n",
    "    for cluster_id, features_1 in clusters_1.items():\n",
    "        best_match = max(clusters_2.items(), key=lambda x: len(set(features_1).intersection(set(x[1]))))\n",
    "        match_percentage = len(set(features_1).intersection(set(best_match[1]))) / len(features_1) * 100\n",
    "        match_percentages.append(match_percentage)\n",
    "    return np.mean(match_percentages) if match_percentages else np.nan\n",
    "\n",
    "def get_detailed_cluster_consistency(clusters_1, clusters_2):\n",
    "    \"\"\"\n",
    "    Für jeden Cluster in clusters_1 wird der beste Matching-Cluster in clusters_2 gesucht.\n",
    "    Gibt ein Dictionary zurück: {orig_cluster: (matched_cluster, match_percentage)}\n",
    "    \"\"\"\n",
    "    details = {}\n",
    "    for cluster_id, features_1 in clusters_1.items():\n",
    "        best_match = max(clusters_2.items(), key=lambda x: len(set(features_1).intersection(set(x[1]))))\n",
    "        match_percentage = len(set(features_1).intersection(set(best_match[1]))) / len(features_1) * 100\n",
    "        details[cluster_id] = (best_match[0], match_percentage)\n",
    "    return details\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Funktion: Gruppenspezifisches Rauschen hinzufügen\n",
    "# -----------------------------\n",
    "def add_noise_to_group(df_group, noise_factor=0.1):\n",
    "    return add_noise_to_data(df_group, noise_factor=noise_factor)\n",
    "\n",
    "# -----------------------------\n",
    "# Simulation: Für eine gegebene Untergruppe\n",
    "# -----------------------------\n",
    "def simulate_state_for_group(df_original, key, noise_factor=0.1, n_reps=100, state=\"global\"):\n",
    "    \"\"\"\n",
    "    Simuliert n_reps Clusterlösungen für eine gegebene Subgruppe (key) im Zustand:\n",
    "      - \"global\": Es wird global Rauschen zum gesamten Datensatz hinzugefügt.\n",
    "      - \"group\": Es wird nur in der Subgruppe Rauschen hinzugefügt.\n",
    "    Gibt eine Liste von Clusterlösungen zurück.\n",
    "    \"\"\"\n",
    "    solutions = []\n",
    "    group_orig = df_original[(df_original[\"Verletzungsstatus\"] == key[0]) & (df_original[\"Geschlecht_weiblich\"] == key[1])]\n",
    "    for _ in range(n_reps):\n",
    "        if state == \"global\":\n",
    "            df_global_noisy = add_noise_to_data(df_original, noise_factor)\n",
    "            group = df_global_noisy[(df_global_noisy[\"Verletzungsstatus\"] == key[0]) & (df_global_noisy[\"Geschlecht_weiblich\"] == key[1])]\n",
    "        elif state == \"group\":\n",
    "            group = add_noise_to_group(group_orig, noise_factor)\n",
    "        else:\n",
    "            raise ValueError(\"state muss 'global' oder 'group' sein.\")\n",
    "        clusters = perform_clustering(group, max_clusters=4)\n",
    "        solutions.append(clusters)\n",
    "    return solutions\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Main-Funktion\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # 5.1 Daten laden\n",
    "    file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\Basistabelle\\Basistabelle_ML_neu.xlsx\"\n",
    "    df_original = pd.read_excel(file_path)\n",
    "    print(\"Original Data Shape:\", df_original.shape, \"\\n\")\n",
    "    \n",
    "    # Baseline: Original Cluster (deterministisch)\n",
    "    clusters_original = get_group_clusters(df_original)\n",
    "    \n",
    "    # --- ERSTER DURCHLAUF: Ausgabe von Variablenlisten und Visualisierungen ---\n",
    "    print(\"==== ERSTER DURCHLAUF: Variablenlisten und Visualisierungen ====\\n\")\n",
    "    \n",
    "    # Global verrauschte Cluster (ein Durchlauf)\n",
    "    df_global_noisy = add_noise_to_data(df_original, noise_factor=0.1)\n",
    "    clusters_global_noisy = get_group_clusters(df_global_noisy)\n",
    "    \n",
    "    # Gruppenspezifisch verrauschte Cluster (ein Durchlauf)\n",
    "    clusters_group_specific = {}\n",
    "    for key, group in df_original.groupby([\"Verletzungsstatus\", \"Geschlecht_weiblich\"]):\n",
    "        group = group.reset_index(drop=True)\n",
    "        group_noisy = add_noise_to_group(group, noise_factor=0.1)\n",
    "        clusters_group_specific[key] = perform_clustering(group_noisy, max_clusters=4)\n",
    "    \n",
    "    # Ausgabe der Clusterzuordnungen (Variablenlisten)\n",
    "    print(\"--- Cluster Zuordnungen: Original ---\\n\")\n",
    "    for key, clusters in clusters_original.items():\n",
    "        print(map_group_label(key))\n",
    "        for cl, cols in clusters.items():\n",
    "            print(\"  Cluster\", cl, \":\", cols)\n",
    "        print(\"\")\n",
    "    \n",
    "    print(\"--- Cluster Zuordnungen: Global Noisy ---\\n\")\n",
    "    for key, clusters in clusters_global_noisy.items():\n",
    "        print(map_group_label(key))\n",
    "        for cl, cols in clusters.items():\n",
    "            print(\"  Cluster\", cl, \":\", cols)\n",
    "        print(\"\")\n",
    "    \n",
    "    print(\"--- Cluster Zuordnungen: Gruppenspezifisch Noisy ---\\n\")\n",
    "    for key, clusters in clusters_group_specific.items():\n",
    "        print(map_group_label(key))\n",
    "        for cl, cols in clusters.items():\n",
    "            print(\"  Cluster\", cl, \":\", cols)\n",
    "        print(\"\")\n",
    "    \n",
    "    # Visualisierung für eine Beispieluntergruppe\n",
    "    example_key = list(clusters_original.keys())[0]\n",
    "    group_orig = df_original[(df_original[\"Verletzungsstatus\"]==example_key[0]) & \n",
    "                             (df_original[\"Geschlecht_weiblich\"]==example_key[1])]\n",
    "    group_global_noisy = df_global_noisy[(df_global_noisy[\"Verletzungsstatus\"]==example_key[0]) & \n",
    "                                         (df_global_noisy[\"Geschlecht_weiblich\"]==example_key[1])]\n",
    "    group_group_noisy = add_noise_to_group(group_orig, noise_factor=0.1)\n",
    "    clusters_example = perform_clustering(group_group_noisy, max_clusters=4)\n",
    "    \n",
    "    visualize_clusters(group_orig, clusters_original[example_key],\n",
    "                         title=f\"Original Cluster ({map_group_label(example_key)})\")\n",
    "    visualize_clusters(group_global_noisy, clusters_global_noisy[example_key],\n",
    "                         title=f\"Global Noisy Cluster ({map_group_label(example_key)})\")\n",
    "    visualize_clusters(group_group_noisy, clusters_example,\n",
    "                         title=f\"Gruppenspezifisch Noisy Cluster ({map_group_label(example_key)})\")\n",
    "    \n",
    "    # --- SIMULATION (50 Wiederholungen) zur Aggregation der Metriken ---\n",
    "    n_reps = 50\n",
    "    sim_results = {}   # Speichert für jede Subgruppe: { 'original': (ARI, NMI, Consistency),\n",
    "                      #  'global': (mean_ARI, std_ARI, mean_NMI, std_NMI, mean_cons, std_cons),\n",
    "                      #  'group': (mean_ARI, std_ARI, mean_NMI, std_NMI, mean_cons, std_cons) }\n",
    "    sim_details = {}   # Für die detaillierte Konsistenz: {key: {comp: {orig_cluster: {'matched': [], 'consistency': []}} } }\n",
    "    \n",
    "    for key in clusters_original.keys():\n",
    "        sim_results[key] = {\n",
    "            'original': (1.0, 0.0, 1.0, 0.0, 100.0, 0.0),\n",
    "            'global': None,\n",
    "            'group': None\n",
    "        }\n",
    "        sim_details[key] = {\n",
    "            'orig_vs_global': {},\n",
    "            'orig_vs_group': {},\n",
    "            'global_vs_group': {}\n",
    "        }\n",
    "        \n",
    "        feature_order = get_feature_order(df_original[(df_original[\"Verletzungsstatus\"]==key[0]) & \n",
    "                                                      (df_original[\"Geschlecht_weiblich\"]==key[1])])\n",
    "        \n",
    "        # Global verrauscht\n",
    "        global_solutions = simulate_state_for_group(df_original, key, 0.1, n_reps, state=\"global\")\n",
    "        ari_g, std_ari_g, nmi_g, std_nmi_g = compute_pairwise_metrics(global_solutions, feature_order)\n",
    "        cons_g, std_cons_g = compute_pairwise_consistency(global_solutions)\n",
    "        \n",
    "        # Gruppenspezifisch verrauscht\n",
    "        group_solutions = simulate_state_for_group(df_original, key, 0.1, n_reps, state=\"group\")\n",
    "        ari_grp, std_ari_grp, nmi_grp, std_nmi_grp = compute_pairwise_metrics(group_solutions, feature_order)\n",
    "        cons_grp, std_cons_grp = compute_pairwise_consistency(group_solutions)\n",
    "        \n",
    "        sim_results[key]['global'] = (ari_g, std_ari_g, nmi_g, std_nmi_g, cons_g, std_cons_g)\n",
    "        sim_results[key]['group'] = (ari_grp, std_ari_grp, nmi_grp, std_nmi_grp, cons_grp, std_cons_grp)\n",
    "        \n",
    "        # Aggregiere detaillierte Konsistenzdaten\n",
    "        for comp, sols in zip(['orig_vs_global', 'orig_vs_group'], [global_solutions, group_solutions]):\n",
    "            details = [get_detailed_cluster_consistency(clusters_original[key], sol) for sol in sols]\n",
    "            # Nun berechnen wir für jeden Originalcluster die Liste der matched Werte und Consistencies\n",
    "            for d in details:\n",
    "                for orig_cluster, (matched, cons) in d.items():\n",
    "                    if orig_cluster not in sim_details[key][comp]:\n",
    "                        sim_details[key][comp][orig_cluster] = {'matched': [], 'consistency': []}\n",
    "                    sim_details[key][comp][orig_cluster]['matched'].append(matched)\n",
    "                    sim_details[key][comp][orig_cluster]['consistency'].append(cons)\n",
    "        \n",
    "        # Für global_vs_group Vergleich\n",
    "        details = [get_detailed_cluster_consistency(sol1, sol2) for sol1, sol2 in zip(global_solutions, group_solutions)]\n",
    "        for d in details:\n",
    "            for orig_cluster, (matched, cons) in d.items():\n",
    "                if orig_cluster not in sim_details[key]['global_vs_group']:\n",
    "                    sim_details[key]['global_vs_group'][orig_cluster] = {'matched': [], 'consistency': []}\n",
    "                sim_details[key]['global_vs_group'][orig_cluster]['matched'].append(matched)\n",
    "                sim_details[key]['global_vs_group'][orig_cluster]['consistency'].append(cons)\n",
    "    \n",
    "    # Aggregierte Ausgabe der stabilitätsmetriken\n",
    "    print(\"=== Aggregierte Stabilitätsmetriken über\", n_reps, \"Wiederholungen ===\\n\")\n",
    "    for key, metrics in sim_results.items():\n",
    "        print(map_group_label(key))\n",
    "        orig = metrics['original']\n",
    "        print(f\"  original:             ARI = {orig[0]:.3f} ± {orig[1]:.3f}, NMI = {orig[2]:.3f} ± {orig[3]:.3f}, Consistency = {orig[4]:.2f}% ± {orig[5]:.2f}%\")\n",
    "        glob = metrics['global']\n",
    "        print(f\"  global verrauscht:    ARI = {glob[0]:.3f} ± {glob[1]:.3f}, NMI = {glob[2]:.3f} ± {glob[3]:.3f}, Consistency = {glob[4]:.2f}% ± {glob[5]:.2f}%\")\n",
    "        grp = metrics['group']\n",
    "        print(f\"  gruppenspezifisch:    ARI = {grp[0]:.3f} ± {grp[1]:.3f}, NMI = {grp[2]:.3f} ± {grp[3]:.3f}, Consistency = {grp[4]:.2f}% ± {grp[5]:.2f}%\")\n",
    "        print(\"\")\n",
    "    \n",
    "    # Aggregierte Ausgabe der detaillierten Cluster-Konsistenz\n",
    "    print(\"=== Aggregierte Detaillierte Cluster-Konsistenz (Übereinstimmungsprozente) ===\\n\")\n",
    "    for key, comp_dict in sim_details.items():\n",
    "        print(map_group_label(key))\n",
    "        for comp, clusters_dict in comp_dict.items():\n",
    "            print(f\"  {comp}:\")\n",
    "            for orig_cluster, values in clusters_dict.items():\n",
    "                cons_mean = np.mean(values['consistency'])\n",
    "                cons_std = np.std(values['consistency'])\n",
    "                mode = Counter(values['matched']).most_common(1)[0][0]\n",
    "                print(f\"    Orig. Cluster {orig_cluster}: {mode} (Übereinstimmung: {cons_mean:.2f}% ± {cons_std:.2f}%)\")\n",
    "            print(\"\")\n",
    "        print(\"\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d6dd4c-925a-4fd4-b76f-edcd3ad93156",
   "metadata": {},
   "source": [
    "Clusterbildung-Validierung mit Rauschen global und gruppenspezifisch, über 100 wiederholungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55af012-4ce2-48f4-a8c1-b4e86d988f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Hilfsfunktion: Gruppenschlüssel übersetzen\n",
    "# -----------------------------\n",
    "def map_group_label(key):\n",
    "    \"\"\"\n",
    "    Übersetzt den Gruppenschlüssel (Verletzungsstatus, Geschlecht_weiblich) \n",
    "    in eine lesbare Bezeichnung.\n",
    "    \n",
    "    Verletzungsstatus: 0 -> \"unverletzt\", 1 -> \"verletzt\"\n",
    "    Geschlecht_weiblich: 0 -> \"männlich\", 1 -> \"weiblich\"\n",
    "    \"\"\"\n",
    "    status = \"verletzt\" if key[0] == 1 else \"unverletzt\"\n",
    "    gender = \"weiblich\" if key[1] == 1 else \"männlich\"\n",
    "    return f\"Gruppe ({status}-{gender})\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Funktion: Rauschen hinzufügen\n",
    "# -----------------------------\n",
    "def add_noise_to_data(df, noise_factor=0.1):\n",
    "    \"\"\"\n",
    "    Fügt allen numerischen Spalten (außer \"Geschlecht_weiblich\" und \"Verletzungsstatus\")\n",
    "    Rauschen hinzu, wobei das Rauschen ein Anteil (noise_factor) der ursprünglichen\n",
    "    Standardabweichung beträgt.\n",
    "    \"\"\"\n",
    "    df_noisy = df.copy()\n",
    "    numeric_cols = [col for col in df_noisy.select_dtypes(include=['float64', 'int64']).columns \n",
    "                    if col not in [\"Geschlecht_weiblich\", \"Verletzungsstatus\"]]\n",
    "    for col in numeric_cols:\n",
    "        std = df_noisy[col].std()\n",
    "        noise = np.random.normal(0, std * noise_factor, size=df_noisy.shape[0])\n",
    "        df_noisy[col] += noise\n",
    "    return df_noisy\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Funktion: Clusterbildung\n",
    "# -----------------------------\n",
    "def perform_clustering(X_sub, max_clusters=4):\n",
    "    \"\"\"\n",
    "    Führt eine hierarchische Clusteranalyse auf den numerischen Features (ohne \n",
    "    \"Geschlecht_weiblich\" und \"Verletzungsstatus\") durch und teilt in max_clusters ein.\n",
    "    \"\"\"\n",
    "    cols = [col for col in X_sub.columns if col not in [\"Verletzungsstatus\", \"Geschlecht_weiblich\"]]\n",
    "    X_num = X_sub[cols]\n",
    "    corr = X_num.corr().abs()\n",
    "    dist = np.clip(1 - corr, a_min=0, a_max=None)\n",
    "    dists = squareform(dist.values)\n",
    "    Z = linkage(dists, method='ward')\n",
    "    cluster_labels = fcluster(Z, max_clusters, criterion='maxclust')\n",
    "    clusters = {}\n",
    "    for col, label in zip(cols, cluster_labels):\n",
    "        clusters.setdefault(label, []).append(col)\n",
    "    return clusters\n",
    "\n",
    "# -----------------------------\n",
    "# Hilfsfunktionen für Stabilitätsmetriken und Visualisierung\n",
    "# -----------------------------\n",
    "def get_feature_order(df):\n",
    "    return [col for col in df.columns if col not in [\"Verletzungsstatus\", \"Geschlecht_weiblich\"]]\n",
    "\n",
    "def get_cluster_labels(clusters, feature_order):\n",
    "    feature_cluster = {}\n",
    "    for cl, feats in clusters.items():\n",
    "        for feat in feats:\n",
    "            feature_cluster[feat] = cl\n",
    "    return [feature_cluster.get(feat, -1) for feat in feature_order]\n",
    "\n",
    "def visualize_clusters(df_group, clusters, title=\"Cluster Visualization\"):\n",
    "    \"\"\"\n",
    "    Visualisiert die Cluster der Features mittels PCA (auf der Korrelationsmatrix).\n",
    "    Es werden keine Textannotationen angezeigt, damit die Grafik übersichtlicher bleibt.\n",
    "    \"\"\"\n",
    "    feature_order = get_feature_order(df_group)\n",
    "    X_num = df_group[feature_order]\n",
    "    corr = X_num.corr().values\n",
    "    pca = PCA(n_components=2)\n",
    "    coords = pca.fit_transform(corr)\n",
    "    labels = get_cluster_labels(clusters, feature_order)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    scatter = plt.scatter(coords[:,0], coords[:,1], c=labels, cmap='viridis', s=100)\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar(scatter, label=\"Cluster\")\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Funktionen für Gruppierung und Vergleich\n",
    "# -----------------------------\n",
    "def get_group_clusters(df):\n",
    "    \"\"\"\n",
    "    Gruppiert den Datensatz anhand von (\"Verletzungsstatus\", \"Geschlecht_weiblich\")\n",
    "    und berechnet für jede Gruppe die Cluster.\n",
    "    \"\"\"\n",
    "    groups = {}\n",
    "    for key, group in df.groupby([\"Verletzungsstatus\", \"Geschlecht_weiblich\"]):\n",
    "        group = group.reset_index(drop=True)\n",
    "        clusters = perform_clustering(group, max_clusters=4)\n",
    "        groups[key] = clusters\n",
    "    return groups\n",
    "\n",
    "def compute_stability_metrics(clusters_1, clusters_2, feature_order):\n",
    "    \"\"\"\n",
    "    Vergleicht zwei Clusterlösungen anhand von ARI und NMI.\n",
    "    \"\"\"\n",
    "    labels_1 = get_cluster_labels(clusters_1, feature_order)\n",
    "    labels_2 = get_cluster_labels(clusters_2, feature_order)\n",
    "    ari = adjusted_rand_score(labels_1, labels_2)\n",
    "    nmi = normalized_mutual_info_score(labels_1, labels_2)\n",
    "    return ari, nmi\n",
    "\n",
    "def compute_pairwise_metrics(solution_list, feature_order):\n",
    "    \"\"\"\n",
    "    Berechnet pairwise ARI und NMI über eine Liste von Clusterlösungen.\n",
    "    Gibt (mean_ARI, std_ARI, mean_NMI, std_NMI) zurück.\n",
    "    \"\"\"\n",
    "    aris = []\n",
    "    nmis = []\n",
    "    n = len(solution_list)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            ari, nmi = compute_stability_metrics(solution_list[i], solution_list[j], feature_order)\n",
    "            aris.append(ari)\n",
    "            nmis.append(nmi)\n",
    "    return np.mean(aris), np.std(aris), np.mean(nmis), np.std(nmis)\n",
    "\n",
    "def compute_pairwise_consistency(solution_list):\n",
    "    \"\"\"\n",
    "    Berechnet pairwise Cluster-Konsistenz (durchschnittlicher Match-Prozentsatz) über eine Liste von Clusterlösungen.\n",
    "    \"\"\"\n",
    "    consistencies = []\n",
    "    n = len(solution_list)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            cons = average_cluster_consistency(solution_list[i], solution_list[j])\n",
    "            consistencies.append(cons)\n",
    "    return np.mean(consistencies), np.std(consistencies)\n",
    "\n",
    "def average_cluster_consistency(clusters_1, clusters_2):\n",
    "    \"\"\"\n",
    "    Berechnet für einen Vergleich zweier Clusterlösungen den durchschnittlichen Match-Prozentsatz.\n",
    "    Für jeden Cluster in clusters_1 wird der beste Matching-Cluster in clusters_2 gesucht.\n",
    "    \"\"\"\n",
    "    match_percentages = []\n",
    "    for cluster_id, features_1 in clusters_1.items():\n",
    "        best_match = max(clusters_2.items(), key=lambda x: len(set(features_1).intersection(set(x[1]))))\n",
    "        match_percentage = len(set(features_1).intersection(set(best_match[1]))) / len(features_1) * 100\n",
    "        match_percentages.append(match_percentage)\n",
    "    return np.mean(match_percentages) if match_percentages else np.nan\n",
    "\n",
    "def get_detailed_cluster_consistency(clusters_1, clusters_2):\n",
    "    \"\"\"\n",
    "    Für jeden Cluster in clusters_1 wird der beste Matching-Cluster in clusters_2 gesucht.\n",
    "    Gibt ein Dictionary zurück: {orig_cluster: (matched_cluster, match_percentage)}\n",
    "    \"\"\"\n",
    "    details = {}\n",
    "    for cluster_id, features_1 in clusters_1.items():\n",
    "        best_match = max(clusters_2.items(), key=lambda x: len(set(features_1).intersection(set(x[1]))))\n",
    "        match_percentage = len(set(features_1).intersection(set(best_match[1]))) / len(features_1) * 100\n",
    "        details[cluster_id] = (best_match[0], match_percentage)\n",
    "    return details\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Funktion: Gruppenspezifisches Rauschen hinzufügen\n",
    "# -----------------------------\n",
    "def add_noise_to_group(df_group, noise_factor=0.1):\n",
    "    return add_noise_to_data(df_group, noise_factor=noise_factor)\n",
    "\n",
    "# -----------------------------\n",
    "# Simulation: Für eine gegebene Untergruppe\n",
    "# -----------------------------\n",
    "def simulate_state_for_group(df_original, key, noise_factor=0.1, n_reps=100, state=\"global\"):\n",
    "    \"\"\"\n",
    "    Simuliert n_reps Clusterlösungen für eine gegebene Subgruppe (key) im Zustand:\n",
    "      - \"global\": Es wird global Rauschen zum gesamten Datensatz hinzugefügt.\n",
    "      - \"group\": Es wird nur in der Subgruppe Rauschen hinzugefügt.\n",
    "    Gibt eine Liste von Clusterlösungen zurück.\n",
    "    \"\"\"\n",
    "    solutions = []\n",
    "    group_orig = df_original[(df_original[\"Verletzungsstatus\"] == key[0]) & (df_original[\"Geschlecht_weiblich\"] == key[1])]\n",
    "    for _ in range(n_reps):\n",
    "        if state == \"global\":\n",
    "            df_global_noisy = add_noise_to_data(df_original, noise_factor)\n",
    "            group = df_global_noisy[(df_global_noisy[\"Verletzungsstatus\"] == key[0]) & (df_global_noisy[\"Geschlecht_weiblich\"] == key[1])]\n",
    "        elif state == \"group\":\n",
    "            group = add_noise_to_group(group_orig, noise_factor)\n",
    "        else:\n",
    "            raise ValueError(\"state muss 'global' oder 'group' sein.\")\n",
    "        clusters = perform_clustering(group, max_clusters=4)\n",
    "        solutions.append(clusters)\n",
    "    return solutions\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Main-Funktion\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # 5.1 Daten laden\n",
    "    file_path = r\"K:\\Team\\Böhmer_Michael\\TSA\\ML\\Basistabelle\\Basistabelle_ML_neu.xlsx\"\n",
    "    df_original = pd.read_excel(file_path)\n",
    "    print(\"Original Data Shape:\", df_original.shape, \"\\n\")\n",
    "    \n",
    "    # Baseline: Original Cluster (deterministisch)\n",
    "    clusters_original = get_group_clusters(df_original)\n",
    "    \n",
    "    # --- ERSTER DURCHLAUF: Ausgabe von Variablenlisten und Visualisierungen ---\n",
    "    print(\"==== ERSTER DURCHLAUF: Variablenlisten und Visualisierungen ====\\n\")\n",
    "    \n",
    "    # Global verrauschte Cluster (ein Durchlauf)\n",
    "    df_global_noisy = add_noise_to_data(df_original, noise_factor=0.1)\n",
    "    clusters_global_noisy = get_group_clusters(df_global_noisy)\n",
    "    \n",
    "    # Gruppenspezifisch verrauschte Cluster (ein Durchlauf)\n",
    "    clusters_group_specific = {}\n",
    "    for key, group in df_original.groupby([\"Verletzungsstatus\", \"Geschlecht_weiblich\"]):\n",
    "        group = group.reset_index(drop=True)\n",
    "        group_noisy = add_noise_to_group(group, noise_factor=0.1)\n",
    "        clusters_group_specific[key] = perform_clustering(group_noisy, max_clusters=4)\n",
    "    \n",
    "    # Ausgabe der Clusterzuordnungen (Variablenlisten)\n",
    "    print(\"--- Cluster Zuordnungen: Original ---\\n\")\n",
    "    for key, clusters in clusters_original.items():\n",
    "        print(map_group_label(key))\n",
    "        for cl, cols in clusters.items():\n",
    "            print(\"  Cluster\", cl, \":\", cols)\n",
    "        print(\"\")\n",
    "    \n",
    "    print(\"--- Cluster Zuordnungen: Global Noisy ---\\n\")\n",
    "    for key, clusters in clusters_global_noisy.items():\n",
    "        print(map_group_label(key))\n",
    "        for cl, cols in clusters.items():\n",
    "            print(\"  Cluster\", cl, \":\", cols)\n",
    "        print(\"\")\n",
    "    \n",
    "    print(\"--- Cluster Zuordnungen: Gruppenspezifisch Noisy ---\\n\")\n",
    "    for key, clusters in clusters_group_specific.items():\n",
    "        print(map_group_label(key))\n",
    "        for cl, cols in clusters.items():\n",
    "            print(\"  Cluster\", cl, \":\", cols)\n",
    "        print(\"\")\n",
    "    \n",
    "    # Visualisierung für eine Beispieluntergruppe\n",
    "    example_key = list(clusters_original.keys())[0]\n",
    "    group_orig = df_original[(df_original[\"Verletzungsstatus\"]==example_key[0]) & \n",
    "                             (df_original[\"Geschlecht_weiblich\"]==example_key[1])]\n",
    "    group_global_noisy = df_global_noisy[(df_global_noisy[\"Verletzungsstatus\"]==example_key[0]) & \n",
    "                                         (df_global_noisy[\"Geschlecht_weiblich\"]==example_key[1])]\n",
    "    group_group_noisy = add_noise_to_group(group_orig, noise_factor=0.1)\n",
    "    clusters_example = perform_clustering(group_group_noisy, max_clusters=4)\n",
    "    \n",
    "    visualize_clusters(group_orig, clusters_original[example_key],\n",
    "                         title=f\"Original Cluster ({map_group_label(example_key)})\")\n",
    "    visualize_clusters(group_global_noisy, clusters_global_noisy[example_key],\n",
    "                         title=f\"Global Noisy Cluster ({map_group_label(example_key)})\")\n",
    "    visualize_clusters(group_group_noisy, clusters_example,\n",
    "                         title=f\"Gruppenspezifisch Noisy Cluster ({map_group_label(example_key)})\")\n",
    "    \n",
    "    # --- SIMULATION (50 Wiederholungen) zur Aggregation der Metriken ---\n",
    "    n_reps = 50\n",
    "    sim_results = {}   # Speichert für jede Subgruppe: { 'original': (ARI, NMI, Consistency),\n",
    "                      #  'global': (mean_ARI, std_ARI, mean_NMI, std_NMI, mean_cons, std_cons),\n",
    "                      #  'group': (mean_ARI, std_ARI, mean_NMI, std_NMI, mean_cons, std_cons) }\n",
    "    sim_details = {}   # Für die detaillierte Konsistenz: {key: {comp: {orig_cluster: {'matched': [], 'consistency': []}} } }\n",
    "    \n",
    "    for key in clusters_original.keys():\n",
    "        sim_results[key] = {\n",
    "            'original': (1.0, 0.0, 1.0, 0.0, 100.0, 0.0),\n",
    "            'global': None,\n",
    "            'group': None\n",
    "        }\n",
    "        sim_details[key] = {\n",
    "            'orig_vs_global': {},\n",
    "            'orig_vs_group': {},\n",
    "            'global_vs_group': {}\n",
    "        }\n",
    "        \n",
    "        feature_order = get_feature_order(df_original[(df_original[\"Verletzungsstatus\"]==key[0]) & \n",
    "                                                      (df_original[\"Geschlecht_weiblich\"]==key[1])])\n",
    "        \n",
    "        # Global verrauscht\n",
    "        global_solutions = simulate_state_for_group(df_original, key, 0.1, n_reps, state=\"global\")\n",
    "        ari_g, std_ari_g, nmi_g, std_nmi_g = compute_pairwise_metrics(global_solutions, feature_order)\n",
    "        cons_g, std_cons_g = compute_pairwise_consistency(global_solutions)\n",
    "        \n",
    "        # Gruppenspezifisch verrauscht\n",
    "        group_solutions = simulate_state_for_group(df_original, key, 0.1, n_reps, state=\"group\")\n",
    "        ari_grp, std_ari_grp, nmi_grp, std_nmi_grp = compute_pairwise_metrics(group_solutions, feature_order)\n",
    "        cons_grp, std_cons_grp = compute_pairwise_consistency(group_solutions)\n",
    "        \n",
    "        sim_results[key]['global'] = (ari_g, std_ari_g, nmi_g, std_nmi_g, cons_g, std_cons_g)\n",
    "        sim_results[key]['group'] = (ari_grp, std_ari_grp, nmi_grp, std_nmi_grp, cons_grp, std_cons_grp)\n",
    "        \n",
    "        # Aggregiere detaillierte Konsistenzdaten\n",
    "        for comp, sols in zip(['orig_vs_global', 'orig_vs_group'], [global_solutions, group_solutions]):\n",
    "            details = [get_detailed_cluster_consistency(clusters_original[key], sol) for sol in sols]\n",
    "            # Nun berechnen wir für jeden Originalcluster die Liste der matched Werte und Consistencies\n",
    "            for d in details:\n",
    "                for orig_cluster, (matched, cons) in d.items():\n",
    "                    if orig_cluster not in sim_details[key][comp]:\n",
    "                        sim_details[key][comp][orig_cluster] = {'matched': [], 'consistency': []}\n",
    "                    sim_details[key][comp][orig_cluster]['matched'].append(matched)\n",
    "                    sim_details[key][comp][orig_cluster]['consistency'].append(cons)\n",
    "        \n",
    "        # Für global_vs_group Vergleich\n",
    "        details = [get_detailed_cluster_consistency(sol1, sol2) for sol1, sol2 in zip(global_solutions, group_solutions)]\n",
    "        for d in details:\n",
    "            for orig_cluster, (matched, cons) in d.items():\n",
    "                if orig_cluster not in sim_details[key]['global_vs_group']:\n",
    "                    sim_details[key]['global_vs_group'][orig_cluster] = {'matched': [], 'consistency': []}\n",
    "                sim_details[key]['global_vs_group'][orig_cluster]['matched'].append(matched)\n",
    "                sim_details[key]['global_vs_group'][orig_cluster]['consistency'].append(cons)\n",
    "    \n",
    "    # Aggregierte Ausgabe der stabilitätsmetriken\n",
    "    print(\"=== Aggregierte Stabilitätsmetriken über\", n_reps, \"Wiederholungen ===\\n\")\n",
    "    for key, metrics in sim_results.items():\n",
    "        print(map_group_label(key))\n",
    "        orig = metrics['original']\n",
    "        print(f\"  original:             ARI = {orig[0]:.3f} ± {orig[1]:.3f}, NMI = {orig[2]:.3f} ± {orig[3]:.3f}, Consistency = {orig[4]:.2f}% ± {orig[5]:.2f}%\")\n",
    "        glob = metrics['global']\n",
    "        print(f\"  global verrauscht:    ARI = {glob[0]:.3f} ± {glob[1]:.3f}, NMI = {glob[2]:.3f} ± {glob[3]:.3f}, Consistency = {glob[4]:.2f}% ± {glob[5]:.2f}%\")\n",
    "        grp = metrics['group']\n",
    "        print(f\"  gruppenspezifisch:    ARI = {grp[0]:.3f} ± {grp[1]:.3f}, NMI = {grp[2]:.3f} ± {grp[3]:.3f}, Consistency = {grp[4]:.2f}% ± {grp[5]:.2f}%\")\n",
    "        print(\"\")\n",
    "    \n",
    "    # Aggregierte Ausgabe der detaillierten Cluster-Konsistenz\n",
    "    print(\"=== Aggregierte Detaillierte Cluster-Konsistenz (Übereinstimmungsprozente) ===\\n\")\n",
    "    for key, comp_dict in sim_details.items():\n",
    "        print(map_group_label(key))\n",
    "        for comp, clusters_dict in comp_dict.items():\n",
    "            print(f\"  {comp}:\")\n",
    "            for orig_cluster, values in clusters_dict.items():\n",
    "                cons_mean = np.mean(values['consistency'])\n",
    "                cons_std = np.std(values['consistency'])\n",
    "                mode = Counter(values['matched']).most_common(1)[0][0]\n",
    "                print(f\"    Orig. Cluster {orig_cluster}: {mode} (Übereinstimmung: {cons_mean:.2f}% ± {cons_std:.2f}%)\")\n",
    "            print(\"\")\n",
    "        print(\"\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0412a5ae-323e-4873-92ca-691fc8e9134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'Test_ML.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    }
   ],
   "source": [
    "!git add Test_ML.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f40e267-8a32-4a67-88cf-2c6f0fd04221",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git remote set-url origin https://github.com/michi1308/Test_ML.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "890ae85b-6fdb-47bd-abb8-261bff61d37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 8a9f928] Datenaugmentation Validierung mit Rauschen\n",
      " 1 file changed, 11 insertions(+), 3 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Datenaugmentation Validierung mit Rauschen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "219f0a89-9581-474f-b559-d5af2f7f4444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   Motum_ML.ipynb\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t.ipynb_checkpoints/\n",
      "\tDatenaugmentation.ipynb\n",
      "\tDatenexport_Skript_2.0.ipynb\n",
      "\tExzentrik_finale_Version.ipynb\n",
      "\tHyperparameter.ipynb\n",
      "\tIsokinetik_finale_Version.ipynb\n",
      "\tIsometrie_finale_Version.ipynb\n",
      "\tML_Mutual_Information_Korrelationsmatrix.ipynb\n",
      "\tMergetabelle_NEU.ipynb\n",
      "\tPCA.ipynb\n",
      "\tPCA_TSA.ipynb\n",
      "\tSprung_Excel_Merge.ipynb\n",
      "\tTSA_grafik.ipynb\n",
      "\tTest.ipynb\n",
      "\tUntitled2.ipynb\n",
      "\tUntitled3.ipynb\n",
      "\tVergleich_TSA_ML.ipynb\n",
      "\talgorithmus_signifikante_werte.ipynb\n",
      "\tbest_model.py\n",
      "\tgit_hub.ipynb\n",
      "\tgradient_boosting_optimieren.ipynb\n",
      "\tmaestroni_faktoren.ipynb\n",
      "\tmaestroni_statistik_deskriptiv.ipynb\n",
      "\tml.ipynb\n",
      "\tml_algorithmus_finden.ipynb\n",
      "\tmotum_ml_feature_selection/\n",
      "\tstacking_tsa.ipynb\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11b0aaa8-d9e1-4785-ae26-7bd66eeeada3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin\thttps://github.com/michi1308/Test_ML.git (fetch)\n",
      "origin\thttps://github.com/michi1308/Test_ML.git (push)\n"
     ]
    }
   ],
   "source": [
    "!git remote -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "deed01ca-2263-4f34-a5da-d0e11cb30653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/michi1308/Test_ML.git\n",
      "   511cd20..8a9f928  master -> master\n"
     ]
    }
   ],
   "source": [
    "!git push origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e929861e-90e0-4ee8-97d1-94e2003d63a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
